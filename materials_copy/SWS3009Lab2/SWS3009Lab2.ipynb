{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b8b8d876",
   "metadata": {},
   "source": [
    "# Lab 2. Building and Using Transformers\n",
    "\n",
    "## 1. Introduction\n",
    "\n",
    "In the previous lab we looked at how to build an LSTM to generate text. In this lab we will look at how to do the same thing with transformers.\n",
    "\n",
    "In the lecture we have already seen how transformers are built in Keras. In this lab rather than building our transformers from scratch, we will use pre-made models on Hugging Face. In the first part we will build a story generator from a Hugging Face template, and in the second part we will look at how to build the same story generator using a Generative Pretrained Transformer (GPT).\n",
    "\n",
    "## 2. Presentation Instructions\n",
    "\n",
    "You will be presenting the contents of <b>Lab2AnsBk.docx</b> in a demo.\n",
    "\n",
    "## 3. Building our Transformer Based Story Generator\n",
    "\n",
    "We will now proceed to build our story generator. As before we begin by loading our text corpus (we are again using Sherlock Holmes). Unlike LSTMs however we can simply present an entire chunk of text to the transformer. However there is an added complication in that the chunks must be of <b>fixed length</b>.\n",
    "\n",
    "### 3.1 Loading our Dataset\n",
    "\n",
    "As before we will use glob to scan the training and testing directory, then loading the files into the dataset. We filter out the sentences that have fewer than 5 words, then convert all the text to lowercase. This part exactly the same as in Lab 1.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "acaf542a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['sherlock/Train\\\\2852-0.txt', 'sherlock/Train\\\\3289-0.txt', 'sherlock/Train\\\\pg2343.txt', 'sherlock/Train\\\\pg2344.txt', 'sherlock/Train\\\\pg2345.txt', 'sherlock/Train\\\\pg2346.txt', 'sherlock/Train\\\\pg2347.txt', 'sherlock/Train\\\\pg2348.txt']\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 19488 examples [00:00, 290252.53 examples/s]\n",
      "Generating test split: 1768 examples [00:00, 43131.60 examples/s]\n",
      "Filter: 100%|██████████| 19488/19488 [00:00<00:00, 530880.42 examples/s]\n",
      "Filter: 100%|██████████| 1768/1768 [00:00<00:00, 340489.90 examples/s]\n",
      "Map: 100%|██████████| 15042/15042 [00:00<00:00, 29074.43 examples/s]\n",
      "Map: 100%|██████████| 1431/1431 [00:00<00:00, 32723.87 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 15042\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1431\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "import glob\n",
    "traindir=\"sherlock/Train/\"\n",
    "testdir=\"sherlock/Test/\"\n",
    "trainlist = [file for file in glob.glob(traindir+\"*.txt\")]\n",
    "testlist = [file for file in glob.glob(testdir+\"*.txt\")]\n",
    "\n",
    "print(trainlist)\n",
    "# Our training files\n",
    "data_files ={\"train\":trainlist,\n",
    "           \"test\":testlist}\n",
    "\n",
    "# Now load the dataset\n",
    "dataset = load_dataset(\"text\", data_files=data_files)\n",
    "\n",
    "# Discard statements with fewer than 5 words\n",
    "min_len = 5\n",
    "dataset = dataset.filter(lambda example: len(example[\"text\"]) >= min_len)\n",
    "\n",
    "# Turn all our text to lowercase\n",
    "def preprocess(example):\n",
    "    return {\"text\":example[\"text\"].lower()}\n",
    "\n",
    "dataset.map(preprocess)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8876b80a",
   "metadata": {},
   "source": [
    "## 3.2 Training the Tokenizer\n",
    "\n",
    "We now tokenize the dataset. This means mapping all the sentences to vectors of integers. However unlike in Lab 1 we will create a custom tokenizer by adapting the gpt2 tokenizer we used in Lab 1.\n",
    "\n",
    "(If you want to train a tokenizer and language model <b><i>from scratch</i></b>, which is very useful for new languages, please see here: https://huggingface.co/blog/how-to-train)\n",
    "\n",
    "We start by loading the gpt2 tokenizer from Hugging Face. Note that transformers require our sentences to be of <b>fixed length</b>, unlike LSMs in Lab 1 that just require the last <i>lookback</i> tokens.\n",
    "\n",
    "### Question 1\n",
    "\n",
    "Explain why transformers train using entire sentences instead of short \"look back\" sentences like LSTM.\n",
    "\n",
    "### Question 2\n",
    "\n",
    "Explain why the sentences used to train the transformers must be of fixed length.\n",
    "\n",
    "<b>Fill the answers to both questions inside the provided answer book</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2e107887",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Y\\.cache\\huggingface\\hub\\models--gpt2. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size:  50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "max_length = 30\n",
    "\n",
    "# Load the model.\n",
    "model_name = \"gpt2\"\n",
    "root_tokenizer = AutoTokenizer.from_pretrained(model_name, padding=True, truncation=True,\n",
    "                                         max_length=max_length)\n",
    "#Specify the padding token\n",
    "root_tokenizer.pad_token = root_tokenizer.eos_token\n",
    "\n",
    "vocab_size = len(root_tokenizer)\n",
    "print(\"Vocab size: \", vocab_size)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47b5cd61",
   "metadata": {},
   "source": [
    "We can now start training the tokenizer using our dataset. To do so we create a Python generator by yielding a batch of <i>steps</i> sentences at a time. We do this because the dataset is too large to be completely loaded in memory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a8fbadf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab size of our new tokenizer:  20262\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('sherlock\\\\tokenizer_config.json',\n",
       " 'sherlock\\\\special_tokens_map.json',\n",
       " 'sherlock\\\\vocab.json',\n",
       " 'sherlock\\\\merges.txt',\n",
       " 'sherlock\\\\added_tokens.json',\n",
       " 'sherlock\\\\tokenizer.json')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create Python generator to return steps sentences at a time\n",
    "# Notice that we use '()' instead of '[]' for our list. This\n",
    "# yields a generator.\n",
    "\n",
    "def return_training_corpus(dataset, steps):\n",
    "    train = dataset[\"train\"]\n",
    "    return (train[i:i+steps][\"text\"] for i in range(0, len(train), steps))\n",
    "\n",
    "\n",
    "# Create the generator that returns 1000 sentences at a time.\n",
    "# We have about 15,000 sentences in the Sherlock dataset\n",
    "\n",
    "gen = return_training_corpus(dataset, 1000)\n",
    "\n",
    "# Now start training the tokenizer\n",
    "tokenizer = root_tokenizer.train_new_from_iterator(gen, vocab_size, \n",
    "                                                   length = len(dataset[\"train\"]))\n",
    "\n",
    "vocab_size = len(tokenizer)\n",
    "print(\"Vocab size of our new tokenizer: \", vocab_size)\n",
    "tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "# Save the tokenizer\n",
    "tokenizer.save_pretrained(\"sherlock\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e0d5ec",
   "metadata": {},
   "source": [
    "### 3.3 Training a Transformer From Scratch\n",
    "\n",
    "We will now start creating and training a transformer from scratch. In the lecture we have seen how to build a transformer using Keras. It is unproductive to do this again, so we will use pre-configured (but untrained) transformers from Hugging Face. \n",
    "\n",
    "There are several things we need to do:\n",
    "\n",
    "    1. Tokenize the entire dataset, creating fixed-length sentences\n",
    "    2. Load up a pre-configured transformer. We are using the pretrained GPT2 language model.\n",
    "    3. Train the pre-configured transformer and save it.\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5db9a9b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 15042/15042 [00:00<00:00, 42015.13 examples/s]\n",
      "Map: 100%|██████████| 1431/1431 [00:00<00:00, 44390.24 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the entire corpus by using map\n",
    "def tokenize(example):\n",
    "    outputs = tokenizer(example[\"text\"], padding=True, truncation=True,\n",
    "                        max_length=max_length, return_overflowing_tokens=True)\n",
    "    \n",
    "    ret_tokens=[]\n",
    "    \n",
    "    for input_ids in outputs[\"input_ids\"]:\n",
    "        ret_tokens.append(input_ids)\n",
    "\n",
    "    # Map requires a dictionary of tokens to be returned. The token entries\n",
    "    # must be called \"input_ids\"\n",
    "    return {\"input_ids\":ret_tokens}\n",
    "\n",
    "# We must set batched = True so that the tokenizer knows how many characters to pad to.\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, remove_columns = dataset[\"train\"].column_names,\n",
    "                               batched=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0802be4",
   "metadata": {},
   "source": [
    "We can now print the lengths of the first 10 sentences to show you that they've all been padded/truncated to the same length, which is the length of the longest statement seen (or at most 30 characters)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71275ac3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Length of sentence  0 :  22\n",
      "Length of sentence  1 :  22\n",
      "Length of sentence  2 :  22\n",
      "Length of sentence  3 :  22\n",
      "Length of sentence  4 :  22\n",
      "Length of sentence  5 :  22\n",
      "Length of sentence  6 :  22\n",
      "Length of sentence  7 :  22\n",
      "Length of sentence  8 :  22\n",
      "Length of sentence  9 :  22\n"
     ]
    }
   ],
   "source": [
    "for i, token in enumerate(tokenized_dataset[\"train\"][\"input_ids\"][:10]):\n",
    "    print(\"Length of sentence \", i, \": \", len(token))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac36c38",
   "metadata": {},
   "source": [
    "Great! Now let's build our transformer. We will create it from an existing GPT-2 transformer, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "818e10b0",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Failed to import transformers.models.gpt2.modeling_tf_gpt2 because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\activations_tf.py:22\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtf_keras\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\n\u001b[0;32m     23\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m (\u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m, \u001b[38;5;167;01mImportError\u001b[39;00m):\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'tf_keras'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32mc:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1560\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1559\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mimportlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mimport_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m.\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mmodule_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;18;43m__name__\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32mc:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\importlib\\__init__.py:90\u001b[0m, in \u001b[0;36mimport_module\u001b[1;34m(name, package)\u001b[0m\n\u001b[0;32m     89\u001b[0m         level \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m---> 90\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_bootstrap\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_gcd_import\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m[\u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpackage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1387\u001b[0m, in \u001b[0;36m_gcd_import\u001b[1;34m(name, package, level)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1360\u001b[0m, in \u001b[0;36m_find_and_load\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1331\u001b[0m, in \u001b[0;36m_find_and_load_unlocked\u001b[1;34m(name, import_)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:935\u001b[0m, in \u001b[0;36m_load_unlocked\u001b[1;34m(spec)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap_external>:995\u001b[0m, in \u001b[0;36mexec_module\u001b[1;34m(self, module)\u001b[0m\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:488\u001b[0m, in \u001b[0;36m_call_with_frames_removed\u001b[1;34m(f, *args, **kwds)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\models\\gpt2\\modeling_tf_gpt2.py:26\u001b[0m\n\u001b[0;32m     24\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m---> 26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mactivations_tf\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m get_tf_activation\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodeling_tf_outputs\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     28\u001b[0m     TFBaseModelOutputWithPastAndCrossAttentions,\n\u001b[0;32m     29\u001b[0m     TFCausalLMOutputWithCrossAttentions,\n\u001b[0;32m     30\u001b[0m     TFSequenceClassifierOutputWithPast,\n\u001b[0;32m     31\u001b[0m )\n",
      "File \u001b[1;32mc:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\activations_tf.py:27\u001b[0m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m parse(keras\u001b[38;5;241m.\u001b[39m__version__)\u001b[38;5;241m.\u001b[39mmajor \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m2\u001b[39m:\n\u001b[1;32m---> 27\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     28\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour currently installed version of Keras is Keras 3, but this is not yet supported in \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     29\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTransformers. Please install the backwards-compatible tf-keras package with \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     30\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`pip install tf-keras`.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     31\u001b[0m         )\n\u001b[0;32m     34\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_gelu\u001b[39m(x):\n",
      "\u001b[1;31mValueError\u001b[0m: Your currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`.",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Bring in the configuration and transformer\u001b[39;00m\n\u001b[1;32m----> 2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m AutoConfig, TFGPT2LMHeadModel\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Load configuration from existing GPT2 network. Set length of sentences,\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# start of sentence and end of sentence tokens\u001b[39;00m\n\u001b[0;32m      7\u001b[0m config \u001b[38;5;241m=\u001b[39m AutoConfig\u001b[38;5;241m.\u001b[39mfrom_pretrained(model_name, \n\u001b[0;32m      8\u001b[0m                                     vocab_size \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(tokenizer), \n\u001b[0;32m      9\u001b[0m                                     n_ctx \u001b[38;5;241m=\u001b[39m max_length,\n\u001b[0;32m     10\u001b[0m                                    bos_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mbos_token_id,\n\u001b[0;32m     11\u001b[0m                                    eos_token_id \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39meos_token_id)\n",
      "File \u001b[1;32m<frozen importlib._bootstrap>:1412\u001b[0m, in \u001b[0;36m_handle_fromlist\u001b[1;34m(module, fromlist, import_, recursive)\u001b[0m\n",
      "File \u001b[1;32mc:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1551\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[0;32m   1550\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module[name])\n\u001b[1;32m-> 1551\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mmodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1553\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodule \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m has no attribute \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1550\u001b[0m, in \u001b[0;36m_LazyModule.__getattr__\u001b[1;34m(self, name)\u001b[0m\n\u001b[0;32m   1548\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_module(name)\n\u001b[0;32m   1549\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_class_to_module\u001b[38;5;241m.\u001b[39mkeys():\n\u001b[1;32m-> 1550\u001b[0m     module \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_module\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_class_to_module\u001b[49m\u001b[43m[\u001b[49m\u001b[43mname\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1551\u001b[0m     value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(module, name)\n\u001b[0;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\transformers\\utils\\import_utils.py:1562\u001b[0m, in \u001b[0;36m_LazyModule._get_module\u001b[1;34m(self, module_name)\u001b[0m\n\u001b[0;32m   1560\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m importlib\u001b[38;5;241m.\u001b[39mimport_module(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m module_name, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m)\n\u001b[0;32m   1561\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m-> 1562\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[0;32m   1563\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFailed to import \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodule_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m because of the following error (look up to see its\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1564\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m traceback):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   1565\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Failed to import transformers.models.gpt2.modeling_tf_gpt2 because of the following error (look up to see its traceback):\nYour currently installed version of Keras is Keras 3, but this is not yet supported in Transformers. Please install the backwards-compatible tf-keras package with `pip install tf-keras`."
     ]
    }
   ],
   "source": [
    "# Bring in the configuration and transformer\n",
    "from transformers import AutoConfig, TFGPT2LMHeadModel\n",
    "\n",
    "# Load configuration from existing GPT2 network. Set length of sentences,\n",
    "# start of sentence and end of sentence tokens\n",
    "\n",
    "config = AutoConfig.from_pretrained(model_name, \n",
    "                                    vocab_size = len(tokenizer), \n",
    "                                    n_ctx = max_length,\n",
    "                                   bos_token_id = tokenizer.bos_token_id,\n",
    "                                   eos_token_id = tokenizer.eos_token_id)\n",
    "\n",
    "# Create the model\n",
    "model = TFGPT2LMHeadModel(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33a2b400",
   "metadata": {},
   "source": [
    "### Question 3.\n",
    "\n",
    "Using the Hugging Face website or otherwise, explain the parameters in AutoConfig.from_pretrained that we have used.\n",
    "\n",
    "Now we are going to start training the new model. Before this we need to create a data collator that will batch the inputs for training. We then convert the dataset into a TensorFlow dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e265b1ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "# mlm = Masked Language Model, where we masked random words and let\n",
    "# the language model infer what it is. We are not doing that here so\n",
    "# we set mlm to False.\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    "\n",
    "batch_size = 16\n",
    "\n",
    "# Create the TensorFlow datasets\n",
    "tf_train_set = tokenized_dataset[\"train\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator)\n",
    "\n",
    "tf_test_set = tokenized_dataset[\"test\"].to_tf_dataset(\n",
    "    columns=[\"attention_mask\", \"input_ids\", \"labels\"],\n",
    "    shuffle=True,\n",
    "    batch_size=batch_size,\n",
    "    collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f81a707d",
   "metadata": {},
   "source": [
    "Now we begin training the Transformer! We will use an Adam optimizer to train for 5 epochs or do early termination\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba08758",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from transformers import AdamWeightDecay\n",
    "import os\n",
    "\n",
    "# tsherlock is the transformer version of the sherlock story generator\n",
    "filename=\"./tsherlock.h5\"\n",
    "\n",
    "earlystop = EarlyStopping(min_delta=0.01, patience=2)\n",
    "\n",
    "# Compile the model \n",
    "optimizer = AdamWeightDecay(learning_rate=2e-5, weight_decay_rate=0.01)\n",
    "model.compile(optimizer=optimizer)\n",
    "\n",
    "# If the weights file exists, load it\n",
    "if os.path.exists(filename):\n",
    "    # Call model build to initialize the model\n",
    "    # variables, so that we can call load_weights\n",
    "    model.build(input_shape=(None, ))\n",
    "    model.load_weights(filename)\n",
    "    \n",
    "# Train the model. This will take a REALLY long time.\n",
    "epochs=5\n",
    "model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=epochs,\n",
    "         callbacks=[earlystop])\n",
    "\n",
    "# Save the weights\n",
    "model.save_weights(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc9ada84",
   "metadata": {},
   "source": [
    "### 3.4 Generating Stories\n",
    "\n",
    "Generating stories using Hugging Face is considerably easier; we can just define a pipeline with our model and tokenizer, and tell it how many words to generate and whether or not to do sampling. \n",
    "\n",
    "We are going to use a text generation pipeline. You can get a complete list of available pipelines here: https://huggingface.co/docs/transformers/main_classes/pipelines\n",
    "\n",
    "Let's try this now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f0a71d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "# Create the pipeline\n",
    "pipe=pipeline(\"text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Number of words to generate\n",
    "num_words = 100\n",
    "\n",
    "# Our seed sentence\n",
    "seed=\"Elementary my dear Watson, \"\n",
    "text = pipe(seed, max_length=num_words, do_sample=True, no_repeat_ngram_size=2)[0]\n",
    "\n",
    "print(\"Generated text: \\n\")\n",
    "print(text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e71825",
   "metadata": {},
   "source": [
    "### 3.5 Fine-tuning a Pretrained Transformer\n",
    "\n",
    "We will now fine-tune a pretrained transformer and compare the speed and results. We begin by bringing in the pretrained model using TFAutoModelForCausalLM, then use fit as usual to train the network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c65aef5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TFAutoModelForCausalLM\n",
    "\n",
    "# The from_pt parameter is to tell the model to convert the weights from PyTorch\n",
    "# format. We will use the same optimizer as before but set a new checkpoint\n",
    "# with a new filename\n",
    "print(\"Training for %d epochs.\" % epochs)\n",
    "\n",
    "pretrained_filename = \"ptsherlock.h5\"\n",
    "\n",
    "pretrained_model = TFAutoModelForCausalLM.from_pretrained(model_name, from_pt=True)\n",
    "pretrained_model.compile(optimizer=optimizer)\n",
    "\n",
    "# If the weights file exists, load it\n",
    "if os.path.exists(pretrained_filename):\n",
    "    pretrained_model.build(input_shape=(None,))\n",
    "    pretrained_model.load_weights(pretrained_filename)\n",
    "\n",
    "pretrained_model.fit(x=tf_train_set, validation_data=tf_test_set, epochs=epochs,\n",
    "         callbacks=[earlystop])\n",
    "\n",
    "pretrained_model.save_weights(pretrained_filename)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "918225cf",
   "metadata": {},
   "source": [
    "As before we start generating our stories so we can compare them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2825672c",
   "metadata": {},
   "outputs": [],
   "source": [
    "pretrained_pipe = pipeline(\"text-generation\", model=pretrained_model, tokenizer=tokenizer)\n",
    "pretrained_text = pretrained_pipe(seed, max_length=num_words, do_sample=True, \n",
    "                               no_repeat_ngram_size=2)[0]\n",
    "\n",
    "print(\"Generated Text from new Transformer: \\n\")\n",
    "print(text)\n",
    "\n",
    "print(\"\\nPretrained Generated Text: \\n\")\n",
    "print(pretrained_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8df53db9",
   "metadata": {},
   "source": [
    "### Question 4\n",
    "\n",
    "Compare the texts generated from the transformer that was trained from scratch versus the transformer that used the pretrained GPT2 weights. Do you see a difference in quality, e.g. fewer \"non-English\" words?\n",
    "\n",
    "## 4. Summary\n",
    "\n",
    "This lab is a follow-up to Lab 1, and here we use transformers to generate stories instead of LSTMs. We started by training a transformer from scratch, and then proceeded to fine-tune a pretrained model.\n",
    "\n",
    "Hugging Face has many, many models that you can work with, and this lab should serve as an introduction. You are encouraged to explore the other models, and how to use train them and use them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f6e3a29-3ef6-42f5-b5ac-143c4cf0de07",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
