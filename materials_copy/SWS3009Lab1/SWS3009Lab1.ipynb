{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "670bdef5-fda5-404b-9c23-217fff9c65aa",
   "metadata": {},
   "source": [
    "## 3. Story Generation with LSTM\n",
    "\n",
    "We start first by looking at how to generate stories using an LSTM.  To do so we need to do two important things with the text:\n",
    "\n",
    "1. We need to tokenize the text, converting the words into integers.\n",
    "2. We need to use an embedding layer before feeding the words to the LSTM.\n",
    "\n",
    "You may do this lab on Google Colab if you wish.\n",
    "\n",
    "We begin by first installing our dependencies. Note that xformers must be installed last or there will be dependency breakages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "65497dee-4820-4986-ba87-3e38ea570862",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: torch in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.3.0)\n",
      "Requirement already satisfied: filelock in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2024.6.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "! pip install --no-cache-dir torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "98702d61-9ce9-40b6-a6d7-ad23d5073def",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Requirement already satisfied: tensorflow in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (2.16.1)\n",
      "Collecting transformers\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/20/5c/244db59e074e80248fdfa60495eeee257e4d97c3df3487df68be30cd60c8/transformers-4.42.3-py3-none-any.whl (9.3 MB)\n",
      "     ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/9.3 MB ? eta -:--:--\n",
      "      --------------------------------------- 0.1/9.3 MB 798.9 kB/s eta 0:00:12\n",
      "      --------------------------------------- 0.1/9.3 MB 607.9 kB/s eta 0:00:16\n",
      "      --------------------------------------- 0.2/9.3 MB 573.4 kB/s eta 0:00:17\n",
      "      --------------------------------------- 0.2/9.3 MB 573.4 kB/s eta 0:00:17\n",
      "      --------------------------------------- 0.2/9.3 MB 573.4 kB/s eta 0:00:17\n",
      "      --------------------------------------- 0.2/9.3 MB 573.4 kB/s eta 0:00:17\n",
      "      --------------------------------------- 0.2/9.3 MB 573.4 kB/s eta 0:00:17\n",
      "      --------------------------------------- 0.2/9.3 MB 573.4 kB/s eta 0:00:17\n",
      "      --------------------------------------- 0.2/9.3 MB 444.3 kB/s eta 0:00:21\n",
      "      --------------------------------------- 0.2/9.3 MB 444.3 kB/s eta 0:00:21\n",
      "      --------------------------------------- 0.2/9.3 MB 444.3 kB/s eta 0:00:21\n",
      "     - -------------------------------------- 0.4/9.3 MB 588.5 kB/s eta 0:00:16\n",
      "     - -------------------------------------- 0.4/9.3 MB 588.5 kB/s eta 0:00:16\n",
      "     - -------------------------------------- 0.5/9.3 MB 655.2 kB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.5/9.3 MB 655.2 kB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.5/9.3 MB 655.2 kB/s eta 0:00:14\n",
      "     -- ------------------------------------- 0.5/9.3 MB 618.8 kB/s eta 0:00:15\n",
      "     -- ------------------------------------- 0.5/9.3 MB 618.8 kB/s eta 0:00:15\n",
      "     -- ------------------------------------- 0.5/9.3 MB 618.8 kB/s eta 0:00:15\n",
      "     -- ------------------------------------- 0.7/9.3 MB 697.5 kB/s eta 0:00:13\n",
      "     --- ------------------------------------ 0.7/9.3 MB 725.7 kB/s eta 0:00:12\n",
      "     --- ------------------------------------ 0.8/9.3 MB 742.4 kB/s eta 0:00:12\n",
      "     --- ------------------------------------ 0.8/9.3 MB 778.9 kB/s eta 0:00:11\n",
      "     --- ------------------------------------ 0.9/9.3 MB 771.8 kB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.0/9.3 MB 821.3 kB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.0/9.3 MB 796.6 kB/s eta 0:00:11\n",
      "     ---- ----------------------------------- 1.1/9.3 MB 855.3 kB/s eta 0:00:10\n",
      "     ----- ---------------------------------- 1.3/9.3 MB 948.4 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 1.3/9.3 MB 960.0 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 1.3/9.3 MB 960.0 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 1.3/9.3 MB 960.0 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 1.3/9.3 MB 906.2 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 1.3/9.3 MB 906.2 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 1.3/9.3 MB 906.2 kB/s eta 0:00:09\n",
      "     ------ --------------------------------- 1.4/9.3 MB 869.6 kB/s eta 0:00:10\n",
      "     ------ --------------------------------- 1.4/9.3 MB 869.6 kB/s eta 0:00:10\n",
      "     ------ --------------------------------- 1.4/9.3 MB 869.6 kB/s eta 0:00:10\n",
      "     ------ --------------------------------- 1.4/9.3 MB 869.6 kB/s eta 0:00:10\n",
      "     ------- -------------------------------- 1.8/9.3 MB 977.4 kB/s eta 0:00:08\n",
      "     ------- -------------------------------- 1.8/9.3 MB 974.7 kB/s eta 0:00:08\n",
      "     ------- -------------------------------- 1.8/9.3 MB 974.7 kB/s eta 0:00:08\n",
      "     ------- -------------------------------- 1.8/9.3 MB 974.7 kB/s eta 0:00:08\n",
      "     ------- -------------------------------- 1.8/9.3 MB 938.5 kB/s eta 0:00:08\n",
      "     -------- ------------------------------- 2.0/9.3 MB 993.3 kB/s eta 0:00:08\n",
      "     -------- ------------------------------- 2.1/9.3 MB 1.0 MB/s eta 0:00:08\n",
      "     -------- ------------------------------- 2.1/9.3 MB 995.4 kB/s eta 0:00:08\n",
      "     -------- ------------------------------- 2.1/9.3 MB 995.4 kB/s eta 0:00:08\n",
      "     -------- ------------------------------- 2.1/9.3 MB 995.4 kB/s eta 0:00:08\n",
      "     --------- ------------------------------ 2.2/9.3 MB 987.9 kB/s eta 0:00:08\n",
      "     --------- ------------------------------ 2.2/9.3 MB 987.9 kB/s eta 0:00:08\n",
      "     --------- ------------------------------ 2.2/9.3 MB 987.9 kB/s eta 0:00:08\n",
      "     ---------- ----------------------------- 2.4/9.3 MB 1.0 MB/s eta 0:00:07\n",
      "     ----------- ---------------------------- 2.6/9.3 MB 1.1 MB/s eta 0:00:07\n",
      "     ----------- ---------------------------- 2.6/9.3 MB 1.1 MB/s eta 0:00:07\n",
      "     ----------- ---------------------------- 2.7/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 2.7/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 2.7/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 2.9/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 2.9/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 2.9/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 2.9/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 2.9/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 2.9/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 2.9/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 2.9/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------ --------------------------- 2.9/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 3.1/9.3 MB 1.0 MB/s eta 0:00:06\n",
      "     ------------- -------------------------- 3.1/9.3 MB 1.0 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 3.4/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     -------------- ------------------------- 3.4/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 3.6/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 3.6/9.3 MB 1.1 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 3.6/9.3 MB 1.1 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 3.6/9.3 MB 1.1 MB/s eta 0:00:05\n",
      "     --------------- ------------------------ 3.7/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 3.7/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 3.7/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 3.7/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 3.7/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     --------------- ------------------------ 3.7/9.3 MB 1.1 MB/s eta 0:00:06\n",
      "     ---------------- ----------------------- 3.9/9.3 MB 1.1 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 4.0/9.3 MB 1.1 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 4.0/9.3 MB 1.1 MB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 4.0/9.3 MB 1.1 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 4.2/9.3 MB 1.1 MB/s eta 0:00:05\n",
      "     ------------------ --------------------- 4.2/9.3 MB 1.1 MB/s eta 0:00:05\n",
      "     ------------------- -------------------- 4.5/9.3 MB 1.2 MB/s eta 0:00:05\n",
      "     -------------------- ------------------- 4.7/9.3 MB 1.2 MB/s eta 0:00:04\n",
      "     -------------------- ------------------- 4.8/9.3 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 4.9/9.3 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 4.9/9.3 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 4.9/9.3 MB 1.2 MB/s eta 0:00:04\n",
      "     --------------------- ------------------ 4.9/9.3 MB 1.2 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 5.2/9.3 MB 1.2 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 5.2/9.3 MB 1.2 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 5.2/9.3 MB 1.2 MB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 5.2/9.3 MB 1.2 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 5.5/9.3 MB 1.3 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 5.5/9.3 MB 1.3 MB/s eta 0:00:04\n",
      "     ----------------------- ---------------- 5.6/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 5.6/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 5.7/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 5.7/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------ --------------- 5.8/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 5.9/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 5.9/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 5.9/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     ------------------------- -------------- 5.9/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 6.3/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 6.3/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     -------------------------- ------------- 6.3/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     --------------------------- ------------ 6.5/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 6.6/9.3 MB 1.3 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 6.7/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 6.8/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 6.9/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 6.9/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 7.0/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 7.0/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 7.0/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     ----------------------------- ---------- 7.0/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 7.3/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 7.4/9.3 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 7.4/9.3 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 7.4/9.3 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 7.4/9.3 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 7.4/9.3 MB 1.4 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 7.4/9.3 MB 1.4 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 7.6/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 7.6/9.3 MB 1.3 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 7.9/9.3 MB 1.4 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 7.9/9.3 MB 1.4 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 8.0/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.0/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.1/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.1/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 8.1/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 8.4/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 8.7/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 8.8/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 8.8/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 8.8/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 8.9/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 8.9/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 8.9/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 8.9/9.3 MB 1.4 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 9.3/9.3 MB 1.4 MB/s eta 0:00:00\n",
      "Collecting datasets\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/60/2d/963b266bb8f88492d5ab4232d74292af8beb5b6fdae97902df9e284d4c32/datasets-2.20.0-py3-none-any.whl (547 kB)\n",
      "     ---------------------------------------- 0.0/547.8 kB ? eta -:--:--\n",
      "     -- ------------------------------------ 41.0/547.8 kB 2.0 MB/s eta 0:00:01\n",
      "     -- ------------------------------------ 41.0/547.8 kB 2.0 MB/s eta 0:00:01\n",
      "     -- ------------------------------------ 41.0/547.8 kB 2.0 MB/s eta 0:00:01\n",
      "     --------- -------------------------- 143.4/547.8 kB 853.3 kB/s eta 0:00:01\n",
      "     --------------- ---------------------- 225.3/547.8 kB 1.1 MB/s eta 0:00:01\n",
      "     --------------- ---------------------- 225.3/547.8 kB 1.1 MB/s eta 0:00:01\n",
      "     --------------- ---------------------- 225.3/547.8 kB 1.1 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 440.3/547.8 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 440.3/547.8 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 440.3/547.8 kB 1.4 MB/s eta 0:00:01\n",
      "     ------------------------------ ------- 440.3/547.8 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 547.8/547.8 kB 1.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: numpy in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.26.4)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.5.0)\n",
      "Requirement already satisfied: tensorflow-intel==2.16.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow) (2.16.1)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.1.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.6.3)\n",
      "Requirement already satisfied: flatbuffers>=23.5.26 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.3.25)\n",
      "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.5.4)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.2.0)\n",
      "Requirement already satisfied: h5py>=3.10.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.11.0)\n",
      "Requirement already satisfied: libclang>=13.0.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (18.1.1)\n",
      "Requirement already satisfied: ml-dtypes~=0.3.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (0.3.2)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.0)\n",
      "Requirement already satisfied: packaging in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (24.0)\n",
      "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.25.3)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.31.0)\n",
      "Requirement already satisfied: setuptools in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (69.5.1)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (4.11.0)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.16.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (1.64.1)\n",
      "Requirement already satisfied: tensorboard<2.17,>=2.16 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (2.16.2)\n",
      "Requirement already satisfied: keras>=3.0.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorflow-intel==2.16.1->tensorflow) (3.3.3)\n",
      "Requirement already satisfied: filelock in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.14.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.23.3)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/8b/ee/05f14a99a81f1a897a9146f3f565efb116ad6412f875f52e895c02666825/regex-2024.5.15-cp312-cp312-win_amd64.whl (268 kB)\n",
      "     ---------------------------------------- 0.0/268.5 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/268.5 kB ? eta -:--:--\n",
      "     - -------------------------------------- 10.2/268.5 kB ? eta -:--:--\n",
      "     -------- ---------------------------- 61.4/268.5 kB 544.7 kB/s eta 0:00:01\n",
      "     ------------------- ---------------- 143.4/268.5 kB 853.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ - 256.0/268.5 kB 1.4 MB/s eta 0:00:01\n",
      "     -------------------------------------- 268.5/268.5 kB 1.3 MB/s eta 0:00:00\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.3)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0c/97/80bff6937e0c67d30c0facacd4f0bcf4254e581aa4995c73cef8c8640e56/tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "     ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "     --- ------------------------------------ 0.2/2.2 MB 12.2 MB/s eta 0:00:01\n",
      "     ---- ----------------------------------- 0.3/2.2 MB 3.4 MB/s eta 0:00:01\n",
      "     ------- -------------------------------- 0.4/2.2 MB 4.0 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.5/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.5/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.5/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "     -------- ------------------------------- 0.5/2.2 MB 3.0 MB/s eta 0:00:01\n",
      "     --------------- ------------------------ 0.8/2.2 MB 2.6 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 0.9/2.2 MB 2.7 MB/s eta 0:00:01\n",
      "     ----------------- ---------------------- 1.0/2.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ------------------- -------------------- 1.1/2.2 MB 2.4 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.2/2.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ---------------------- ----------------- 1.2/2.2 MB 2.5 MB/s eta 0:00:01\n",
      "     ----------------------- ---------------- 1.3/2.2 MB 2.3 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.4/2.2 MB 2.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.4/2.2 MB 2.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.4/2.2 MB 2.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.4/2.2 MB 2.2 MB/s eta 0:00:01\n",
      "     ------------------------ --------------- 1.4/2.2 MB 2.2 MB/s eta 0:00:01\n",
      "     ---------------------------- ----------- 1.6/2.2 MB 1.9 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------ --------- 1.7/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 1.8/2.2 MB 2.0 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 2.1/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------  2.2/2.2 MB 2.1 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.2/2.2 MB 2.1 MB/s eta 0:00:00\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.4)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fa/2b/a0053f1304586f2976cb2c37ddb0e52cf4114220e805ebba272a1e231ccc/pyarrow-16.1.0-cp312-cp312-win_amd64.whl (25.8 MB)\n",
      "     ---------------------------------------- 0.0/25.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/25.8 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/25.8 MB ? eta -:--:--\n",
      "     --------------------------------------- 0.1/25.8 MB 465.5 kB/s eta 0:00:56\n",
      "     --------------------------------------- 0.1/25.8 MB 465.5 kB/s eta 0:00:56\n",
      "     --------------------------------------- 0.1/25.8 MB 465.5 kB/s eta 0:00:56\n",
      "     --------------------------------------- 0.1/25.8 MB 479.3 kB/s eta 0:00:54\n",
      "      --------------------------------------- 0.4/25.8 MB 1.4 MB/s eta 0:00:18\n",
      "      --------------------------------------- 0.4/25.8 MB 1.4 MB/s eta 0:00:18\n",
      "      --------------------------------------- 0.6/25.8 MB 1.5 MB/s eta 0:00:18\n",
      "     - -------------------------------------- 0.7/25.8 MB 1.5 MB/s eta 0:00:17\n",
      "     - -------------------------------------- 0.8/25.8 MB 1.7 MB/s eta 0:00:15\n",
      "     - -------------------------------------- 0.8/25.8 MB 1.7 MB/s eta 0:00:15\n",
      "     - -------------------------------------- 0.9/25.8 MB 1.7 MB/s eta 0:00:16\n",
      "     - -------------------------------------- 1.0/25.8 MB 1.7 MB/s eta 0:00:15\n",
      "     - -------------------------------------- 1.1/25.8 MB 1.7 MB/s eta 0:00:15\n",
      "     - -------------------------------------- 1.1/25.8 MB 1.7 MB/s eta 0:00:15\n",
      "     - -------------------------------------- 1.1/25.8 MB 1.7 MB/s eta 0:00:15\n",
      "     - -------------------------------------- 1.3/25.8 MB 1.6 MB/s eta 0:00:16\n",
      "     - -------------------------------------- 1.3/25.8 MB 1.6 MB/s eta 0:00:16\n",
      "     -- ------------------------------------- 1.3/25.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -- ------------------------------------- 1.3/25.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -- ------------------------------------- 1.3/25.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -- ------------------------------------- 1.3/25.8 MB 1.5 MB/s eta 0:00:17\n",
      "     -- ------------------------------------- 1.8/25.8 MB 1.8 MB/s eta 0:00:14\n",
      "     -- ------------------------------------- 1.9/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     -- ------------------------------------- 1.9/25.8 MB 1.7 MB/s eta 0:00:15\n",
      "     -- ------------------------------------- 1.9/25.8 MB 1.7 MB/s eta 0:00:15\n",
      "     --- ------------------------------------ 2.1/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 2.1/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 2.1/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 2.1/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 2.3/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 2.3/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 2.5/25.8 MB 1.8 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 2.5/25.8 MB 1.8 MB/s eta 0:00:14\n",
      "     --- ------------------------------------ 2.5/25.8 MB 1.8 MB/s eta 0:00:14\n",
      "     ---- ----------------------------------- 2.6/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     ---- ----------------------------------- 2.7/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     ---- ----------------------------------- 3.0/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ---- ----------------------------------- 3.1/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ---- ----------------------------------- 3.1/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ---- ----------------------------------- 3.2/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ---- ----------------------------------- 3.2/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ---- ----------------------------------- 3.2/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     ----- ---------------------------------- 3.3/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     ----- ---------------------------------- 3.3/25.8 MB 1.7 MB/s eta 0:00:14\n",
      "     ----- ---------------------------------- 3.6/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 3.7/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 3.7/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 3.7/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ----- ---------------------------------- 3.7/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 3.9/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 3.9/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 3.9/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 4.2/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 4.3/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 4.3/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 4.3/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 4.3/25.8 MB 1.8 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 4.3/25.8 MB 1.7 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 4.3/25.8 MB 1.7 MB/s eta 0:00:13\n",
      "     ------ --------------------------------- 4.3/25.8 MB 1.7 MB/s eta 0:00:13\n",
      "     ------- -------------------------------- 4.6/25.8 MB 1.7 MB/s eta 0:00:13\n",
      "     ------- -------------------------------- 4.7/25.8 MB 1.7 MB/s eta 0:00:13\n",
      "     ------- -------------------------------- 5.0/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     ------- -------------------------------- 5.0/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     ------- -------------------------------- 5.0/25.8 MB 1.7 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 5.3/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 5.4/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 5.5/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 5.5/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 5.6/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 5.6/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 5.6/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 5.7/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 5.8/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     -------- ------------------------------- 5.8/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     --------- ------------------------------ 6.0/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     --------- ------------------------------ 6.0/25.8 MB 1.8 MB/s eta 0:00:12\n",
      "     --------- ------------------------------ 6.3/25.8 MB 1.8 MB/s eta 0:00:11\n",
      "     --------- ------------------------------ 6.4/25.8 MB 1.8 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 6.5/25.8 MB 1.9 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 6.5/25.8 MB 1.8 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 6.5/25.8 MB 1.8 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 6.8/25.8 MB 1.9 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 6.9/25.8 MB 1.9 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 7.0/25.8 MB 1.9 MB/s eta 0:00:11\n",
      "     ---------- ----------------------------- 7.1/25.8 MB 1.9 MB/s eta 0:00:11\n",
      "     ----------- ---------------------------- 7.2/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 7.2/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 7.5/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 7.5/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 7.5/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 7.5/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 7.6/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ----------- ---------------------------- 7.6/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------ --------------------------- 7.9/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------ --------------------------- 7.9/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------ --------------------------- 7.9/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------ --------------------------- 8.3/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------ --------------------------- 8.4/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------ --------------------------- 8.4/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------ --------------------------- 8.4/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------- -------------------------- 8.5/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------- -------------------------- 8.5/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------- -------------------------- 8.5/25.8 MB 1.9 MB/s eta 0:00:10\n",
      "     ------------- -------------------------- 8.5/25.8 MB 1.8 MB/s eta 0:00:10\n",
      "     ------------- -------------------------- 8.9/25.8 MB 1.9 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 9.2/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 9.3/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 9.5/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     -------------- ------------------------- 9.6/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 9.7/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 9.8/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 9.8/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 9.8/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 9.8/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 9.8/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 10.1/25.8 MB 2.0 MB/s eta 0:00:09\n",
      "     --------------- ------------------------ 10.2/25.8 MB 2.0 MB/s eta 0:00:08\n",
      "     --------------- ------------------------ 10.3/25.8 MB 2.1 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 10.4/25.8 MB 2.1 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 10.5/25.8 MB 2.0 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 10.8/25.8 MB 2.1 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 10.8/25.8 MB 2.1 MB/s eta 0:00:08\n",
      "     ---------------- ----------------------- 10.8/25.8 MB 2.1 MB/s eta 0:00:08\n",
      "     ----------------- ---------------------- 11.0/25.8 MB 2.1 MB/s eta 0:00:08\n",
      "     ----------------- ---------------------- 11.0/25.8 MB 2.1 MB/s eta 0:00:08\n",
      "     ----------------- ---------------------- 11.4/25.8 MB 2.2 MB/s eta 0:00:07\n",
      "     ----------------- ---------------------- 11.4/25.8 MB 2.1 MB/s eta 0:00:07\n",
      "     ----------------- ---------------------- 11.4/25.8 MB 2.1 MB/s eta 0:00:07\n",
      "     ----------------- ---------------------- 11.4/25.8 MB 2.1 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 11.7/25.8 MB 2.2 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 11.7/25.8 MB 2.2 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 11.7/25.8 MB 2.2 MB/s eta 0:00:07\n",
      "     ------------------ --------------------- 11.7/25.8 MB 2.2 MB/s eta 0:00:07\n",
      "     ------------------- -------------------- 12.3/25.8 MB 2.2 MB/s eta 0:00:07\n",
      "     ------------------- -------------------- 12.4/25.8 MB 2.2 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.6/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.6/25.8 MB 2.2 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.7/25.8 MB 2.2 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.8/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.8/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.8/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.8/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     ------------------- -------------------- 12.8/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 13.5/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 13.5/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 13.5/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     -------------------- ------------------- 13.5/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 13.6/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 13.8/25.8 MB 2.3 MB/s eta 0:00:06\n",
      "     --------------------- ------------------ 14.1/25.8 MB 2.4 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 14.1/25.8 MB 2.4 MB/s eta 0:00:05\n",
      "     --------------------- ------------------ 14.2/25.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 14.3/25.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 14.4/25.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 14.5/25.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 14.5/25.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ---------------------- ----------------- 14.5/25.8 MB 2.3 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 15.0/25.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 15.0/25.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 15.0/25.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 15.0/25.8 MB 2.5 MB/s eta 0:00:05\n",
      "     ----------------------- ---------------- 15.1/25.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ------------------------ --------------- 15.5/25.8 MB 2.4 MB/s eta 0:00:05\n",
      "     ------------------------ --------------- 15.8/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 15.8/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 16.1/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------------------ --------------- 16.1/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 16.3/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 16.3/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 16.3/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 16.3/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     ------------------------- -------------- 16.6/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 16.9/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 17.2/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     -------------------------- ------------- 17.2/25.8 MB 2.6 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 17.6/25.8 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 17.6/25.8 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 17.6/25.8 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 17.6/25.8 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 17.6/25.8 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 17.6/25.8 MB 2.7 MB/s eta 0:00:04\n",
      "     --------------------------- ------------ 18.0/25.8 MB 2.7 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 18.5/25.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 18.5/25.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 18.5/25.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ---------------------------- ----------- 18.7/25.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 19.0/25.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 19.0/25.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 19.0/25.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 19.0/25.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ----------------------------- ---------- 19.0/25.8 MB 2.8 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 19.8/25.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 19.8/25.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------------------ --------- 19.8/25.8 MB 2.9 MB/s eta 0:00:03\n",
      "     ------------------------------- -------- 20.1/25.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 20.1/25.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 20.1/25.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 20.1/25.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 20.1/25.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 20.1/25.8 MB 3.1 MB/s eta 0:00:02\n",
      "     ------------------------------- -------- 20.1/25.8 MB 3.1 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 20.9/25.8 MB 3.0 MB/s eta 0:00:02\n",
      "     -------------------------------- ------- 20.9/25.8 MB 3.0 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 21.5/25.8 MB 3.0 MB/s eta 0:00:02\n",
      "     --------------------------------- ------ 21.6/25.8 MB 3.0 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 22.0/25.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 22.1/25.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ---------------------------------- ----- 22.1/25.8 MB 3.3 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 22.6/25.8 MB 3.2 MB/s eta 0:00:02\n",
      "     ----------------------------------- ---- 22.8/25.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 23.0/25.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 23.1/25.8 MB 3.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 23.1/25.8 MB 3.5 MB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 23.1/25.8 MB 3.5 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.3/25.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.3/25.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.3/25.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.3/25.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.3/25.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------ --- 23.3/25.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 23.9/25.8 MB 3.2 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 24.1/25.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 24.1/25.8 MB 3.3 MB/s eta 0:00:01\n",
      "     ------------------------------------- -- 24.5/25.8 MB 3.3 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 24.7/25.8 MB 3.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 25.1/25.8 MB 3.5 MB/s eta 0:00:01\n",
      "     -------------------------------------- - 25.1/25.8 MB 3.5 MB/s eta 0:00:01\n",
      "     ---------------------------------------  25.7/25.8 MB 3.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 25.8/25.8 MB 3.6 MB/s eta 0:00:00\n",
      "Collecting pyarrow-hotfix (from datasets)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e4/f4/9ec2222f5f5f8ea04f66f184caafd991a39c8782e31f5b0266f101cb68ca/pyarrow_hotfix-0.6-py3-none-any.whl (7.9 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/c9/7a/cef76fd8438a42f96db64ddaa85280485a9c395e7df3db8158cfec1eee34/dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "     ---------------------------------------- 0.0/116.3 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 61.4/116.3 kB ? eta -:--:--\n",
      "     --------------------- ------------------ 61.4/116.3 kB ? eta -:--:--\n",
      "     ----------------------------- ------- 92.2/116.3 kB 871.5 kB/s eta 0:00:01\n",
      "     ----------------------------- ------- 92.2/116.3 kB 871.5 kB/s eta 0:00:01\n",
      "     ------------------------------------ 116.3/116.3 kB 522.2 kB/s eta 0:00:00\n",
      "Requirement already satisfied: pandas in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (2.2.2)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow-intel==2.16.1->tensorflow)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/f9/9b/335f9764261e915ed497fcdeb11df5dfd6f7bf257d4a6a2a686d80da4d54/requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "     ---------------------------------------- 0.0/64.9 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/64.9 kB ? eta -:--:--\n",
      "     ------ --------------------------------- 10.2/64.9 kB ? eta -:--:--\n",
      "     ----------------- -------------------- 30.7/64.9 kB 163.8 kB/s eta 0:00:01\n",
      "     ----------------- -------------------- 30.7/64.9 kB 163.8 kB/s eta 0:00:01\n",
      "     ----------------- -------------------- 30.7/64.9 kB 163.8 kB/s eta 0:00:01\n",
      "     -------------------------------------- 64.9/64.9 kB 233.7 kB/s eta 0:00:00\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/e3/63/2627198c4c9d1f987390043bb352fef9e754ed2b11fd21b40bf430b2714e/xxhash-3.4.1-cp312-cp312-win_amd64.whl (29 kB)\n",
      "Collecting multiprocess (from datasets)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/0a/7d/a988f258104dcd2ccf1ed40fdc97e26c4ac351eeaf81d76e266c52d84e2f/multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "     ---------------------------------------- 0.0/146.7 kB ? eta -:--:--\n",
      "     ----------- ---------------------------- 41.0/146.7 kB ? eta -:--:--\n",
      "     --------------- --------------------- 61.4/146.7 kB 812.7 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/146.7 kB 812.7 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/146.7 kB 812.7 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/146.7 kB 812.7 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/146.7 kB 812.7 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/146.7 kB 812.7 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/146.7 kB 812.7 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/146.7 kB 812.7 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/146.7 kB 812.7 kB/s eta 0:00:01\n",
      "     --------------- --------------------- 61.4/146.7 kB 812.7 kB/s eta 0:00:01\n",
      "     ------------------------------------ 146.7/146.7 kB 264.5 kB/s eta 0:00:00\n",
      "Collecting fsspec<=2024.5.0,>=2023.1.0 (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets)\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/ba/a3/16e9fe32187e9c8bc7f9b7bcd9728529faa725231a0c96f2f98714ff2fc5/fsspec-2024.5.0-py3-none-any.whl (316 kB)\n",
      "     ---------------------------------------- 0.0/316.1 kB ? eta -:--:--\n",
      "     ------------- ------------------------ 112.6/316.1 kB 6.8 MB/s eta 0:00:01\n",
      "     ------------- ------------------------ 112.6/316.1 kB 6.8 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 153.6/316.1 kB 1.5 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 153.6/316.1 kB 1.5 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 153.6/316.1 kB 1.5 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 153.6/316.1 kB 1.5 MB/s eta 0:00:01\n",
      "     ------------------ ------------------- 153.6/316.1 kB 1.5 MB/s eta 0:00:01\n",
      "     ------------------------- ---------- 225.3/316.1 kB 655.6 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 225.3/316.1 kB 655.6 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 225.3/316.1 kB 655.6 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 225.3/316.1 kB 655.6 kB/s eta 0:00:01\n",
      "     ------------------------- ---------- 225.3/316.1 kB 655.6 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 256.0/316.1 kB 462.8 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 256.0/316.1 kB 462.8 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 256.0/316.1 kB 462.8 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 256.0/316.1 kB 462.8 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 256.0/316.1 kB 462.8 kB/s eta 0:00:01\n",
      "     ----------------------------- ------ 256.0/316.1 kB 462.8 kB/s eta 0:00:01\n",
      "     ------------------------------------ 316.1/316.1 kB 391.6 kB/s eta 0:00:00\n",
      "Requirement already satisfied: aiohttp in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: scipy>=1.6.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from scikit-learn) (3.5.0)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.3.1)\n",
      "Requirement already satisfied: attrs>=17.3.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (23.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.4.1)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (6.0.5)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from aiohttp->datasets) (1.9.4)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests<3,>=2.21.0->tensorflow-intel==2.16.1->tensorflow) (2024.2.2)\n",
      "Requirement already satisfied: colorama in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from astunparse>=1.6.0->tensorflow-intel==2.16.1->tensorflow) (0.43.0)\n",
      "Requirement already satisfied: rich in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (13.7.1)\n",
      "Requirement already satisfied: namex in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.0.8)\n",
      "Requirement already satisfied: optree in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.11.0)\n",
      "Requirement already satisfied: markdown>=2.6.8 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.6)\n",
      "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (0.7.2)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (3.0.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.17,>=2.16->tensorflow-intel==2.16.1->tensorflow) (2.1.5)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (3.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (2.18.0)\n",
      "Requirement already satisfied: mdurl~=0.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from markdown-it-py>=2.2.0->rich->keras>=3.0.0->tensorflow-intel==2.16.1->tensorflow) (0.1.2)\n",
      "Installing collected packages: xxhash, requests, regex, pyarrow-hotfix, pyarrow, fsspec, dill, multiprocess, tokenizers, transformers, datasets\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.31.0\n",
      "    Uninstalling requests-2.31.0:\n",
      "      Successfully uninstalled requests-2.31.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2024.6.0\n",
      "    Uninstalling fsspec-2024.6.0:\n",
      "      Successfully uninstalled fsspec-2024.6.0\n",
      "Successfully installed datasets-2.20.0 dill-0.3.8 fsspec-2024.5.0 multiprocess-0.70.16 pyarrow-16.1.0 pyarrow-hotfix-0.6 regex-2024.5.15 requests-2.32.3 tokenizers-0.19.1 transformers-4.42.3 xxhash-3.4.1\n"
     ]
    }
   ],
   "source": [
    "! pip install --no-cache-dir tensorflow transformers datasets numpy scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1fc487da-48cc-458c-81dd-df4c0e284cc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.tuna.tsinghua.edu.cn/simple\n",
      "Collecting xformers\n",
      "  Downloading https://pypi.tuna.tsinghua.edu.cn/packages/fc/d3/5eab1cb3171fc9d4e5613025e965b72727f6d12ae820d2d7ab9c37320eb6/xformers-0.0.26.post1.tar.gz (4.1 MB)\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     ---------------------------------------- 0.0/4.1 MB ? eta -:--:--\n",
      "     - -------------------------------------- 0.1/4.1 MB 297.7 kB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.1/4.1 MB 297.7 kB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.1/4.1 MB 297.7 kB/s eta 0:00:14\n",
      "     - -------------------------------------- 0.2/4.1 MB 316.5 kB/s eta 0:00:13\n",
      "     - -------------------------------------- 0.2/4.1 MB 316.5 kB/s eta 0:00:13\n",
      "     - -------------------------------------- 0.2/4.1 MB 316.5 kB/s eta 0:00:13\n",
      "     - -------------------------------------- 0.2/4.1 MB 316.5 kB/s eta 0:00:13\n",
      "     - -------------------------------------- 0.2/4.1 MB 316.5 kB/s eta 0:00:13\n",
      "     -- ------------------------------------- 0.2/4.1 MB 335.5 kB/s eta 0:00:12\n",
      "     -- ------------------------------------- 0.3/4.1 MB 401.9 kB/s eta 0:00:10\n",
      "     -- ------------------------------------- 0.3/4.1 MB 401.9 kB/s eta 0:00:10\n",
      "     --- ------------------------------------ 0.3/4.1 MB 385.4 kB/s eta 0:00:10\n",
      "     --- ------------------------------------ 0.3/4.1 MB 403.1 kB/s eta 0:00:10\n",
      "     --- ------------------------------------ 0.4/4.1 MB 416.7 kB/s eta 0:00:09\n",
      "     --- ------------------------------------ 0.4/4.1 MB 425.2 kB/s eta 0:00:09\n",
      "     --- ------------------------------------ 0.4/4.1 MB 425.2 kB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 0.4/4.1 MB 423.3 kB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 0.4/4.1 MB 423.3 kB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 0.5/4.1 MB 402.5 kB/s eta 0:00:10\n",
      "     ---- ----------------------------------- 0.5/4.1 MB 424.4 kB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 0.5/4.1 MB 424.4 kB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 0.5/4.1 MB 424.4 kB/s eta 0:00:09\n",
      "     ---- ----------------------------------- 0.5/4.1 MB 424.4 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 0.6/4.1 MB 418.4 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 0.6/4.1 MB 418.4 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 0.6/4.1 MB 418.4 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 0.6/4.1 MB 418.4 kB/s eta 0:00:09\n",
      "     ----- ---------------------------------- 0.6/4.1 MB 418.4 kB/s eta 0:00:09\n",
      "     ------- -------------------------------- 0.7/4.1 MB 491.5 kB/s eta 0:00:07\n",
      "     ------- -------------------------------- 0.7/4.1 MB 491.5 kB/s eta 0:00:07\n",
      "     ------- -------------------------------- 0.8/4.1 MB 475.4 kB/s eta 0:00:07\n",
      "     ------- -------------------------------- 0.8/4.1 MB 475.8 kB/s eta 0:00:07\n",
      "     ------- -------------------------------- 0.8/4.1 MB 475.8 kB/s eta 0:00:07\n",
      "     ------- -------------------------------- 0.8/4.1 MB 475.8 kB/s eta 0:00:07\n",
      "     ------- -------------------------------- 0.8/4.1 MB 475.8 kB/s eta 0:00:07\n",
      "     ------- -------------------------------- 0.8/4.1 MB 475.8 kB/s eta 0:00:07\n",
      "     ------- -------------------------------- 0.8/4.1 MB 475.8 kB/s eta 0:00:07\n",
      "     -------- ------------------------------- 0.9/4.1 MB 476.5 kB/s eta 0:00:07\n",
      "     -------- ------------------------------- 0.9/4.1 MB 476.5 kB/s eta 0:00:07\n",
      "     --------- ------------------------------ 1.0/4.1 MB 491.5 kB/s eta 0:00:07\n",
      "     --------- ------------------------------ 1.0/4.1 MB 491.5 kB/s eta 0:00:07\n",
      "     --------- ------------------------------ 1.0/4.1 MB 491.5 kB/s eta 0:00:07\n",
      "     ---------- ----------------------------- 1.1/4.1 MB 491.5 kB/s eta 0:00:07\n",
      "     ---------- ----------------------------- 1.1/4.1 MB 491.5 kB/s eta 0:00:07\n",
      "     ----------- ---------------------------- 1.2/4.1 MB 520.6 kB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 1.2/4.1 MB 510.6 kB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 1.2/4.1 MB 515.2 kB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 1.2/4.1 MB 515.2 kB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 1.2/4.1 MB 515.2 kB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 1.2/4.1 MB 515.2 kB/s eta 0:00:06\n",
      "     ----------- ---------------------------- 1.2/4.1 MB 515.2 kB/s eta 0:00:06\n",
      "     ------------ --------------------------- 1.3/4.1 MB 512.8 kB/s eta 0:00:06\n",
      "     ------------ --------------------------- 1.3/4.1 MB 512.8 kB/s eta 0:00:06\n",
      "     ------------- -------------------------- 1.4/4.1 MB 533.6 kB/s eta 0:00:06\n",
      "     ------------- -------------------------- 1.4/4.1 MB 538.3 kB/s eta 0:00:05\n",
      "     -------------- ------------------------- 1.5/4.1 MB 544.2 kB/s eta 0:00:05\n",
      "     -------------- ------------------------- 1.5/4.1 MB 553.6 kB/s eta 0:00:05\n",
      "     -------------- ------------------------- 1.5/4.1 MB 553.6 kB/s eta 0:00:05\n",
      "     --------------- ------------------------ 1.6/4.1 MB 577.9 kB/s eta 0:00:05\n",
      "     --------------- ------------------------ 1.6/4.1 MB 577.9 kB/s eta 0:00:05\n",
      "     --------------- ------------------------ 1.6/4.1 MB 577.9 kB/s eta 0:00:05\n",
      "     --------------- ------------------------ 1.6/4.1 MB 577.9 kB/s eta 0:00:05\n",
      "     --------------- ------------------------ 1.6/4.1 MB 577.9 kB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 1.7/4.1 MB 550.0 kB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 1.7/4.1 MB 544.9 kB/s eta 0:00:05\n",
      "     ---------------- ----------------------- 1.7/4.1 MB 558.9 kB/s eta 0:00:05\n",
      "     ------------------ --------------------- 1.8/4.1 MB 577.9 kB/s eta 0:00:04\n",
      "     ------------------ --------------------- 1.8/4.1 MB 577.9 kB/s eta 0:00:04\n",
      "     ------------------ --------------------- 1.8/4.1 MB 577.9 kB/s eta 0:00:04\n",
      "     ------------------ --------------------- 1.9/4.1 MB 574.1 kB/s eta 0:00:04\n",
      "     ------------------ --------------------- 1.9/4.1 MB 572.3 kB/s eta 0:00:04\n",
      "     ------------------- -------------------- 2.0/4.1 MB 597.9 kB/s eta 0:00:04\n",
      "     -------------------- ------------------- 2.1/4.1 MB 598.5 kB/s eta 0:00:04\n",
      "     -------------------- ------------------- 2.1/4.1 MB 619.5 kB/s eta 0:00:04\n",
      "     -------------------- ------------------- 2.1/4.1 MB 619.5 kB/s eta 0:00:04\n",
      "     --------------------- ------------------ 2.2/4.1 MB 614.9 kB/s eta 0:00:04\n",
      "     --------------------- ------------------ 2.2/4.1 MB 614.9 kB/s eta 0:00:04\n",
      "     --------------------- ------------------ 2.2/4.1 MB 614.9 kB/s eta 0:00:04\n",
      "     ---------------------- ----------------- 2.3/4.1 MB 611.1 kB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 2.3/4.1 MB 611.1 kB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 2.3/4.1 MB 611.1 kB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 2.3/4.1 MB 611.1 kB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 2.3/4.1 MB 611.1 kB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 2.3/4.1 MB 611.1 kB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 2.3/4.1 MB 611.1 kB/s eta 0:00:03\n",
      "     ---------------------- ----------------- 2.3/4.1 MB 611.1 kB/s eta 0:00:03\n",
      "     ------------------------ --------------- 2.5/4.1 MB 629.6 kB/s eta 0:00:03\n",
      "     ------------------------ --------------- 2.5/4.1 MB 625.0 kB/s eta 0:00:03\n",
      "     ------------------------ --------------- 2.5/4.1 MB 625.0 kB/s eta 0:00:03\n",
      "     ------------------------- -------------- 2.6/4.1 MB 630.3 kB/s eta 0:00:03\n",
      "     ------------------------- -------------- 2.7/4.1 MB 635.5 kB/s eta 0:00:03\n",
      "     -------------------------- ------------- 2.7/4.1 MB 648.0 kB/s eta 0:00:03\n",
      "     --------------------------- ------------ 2.8/4.1 MB 657.7 kB/s eta 0:00:02\n",
      "     --------------------------- ------------ 2.8/4.1 MB 655.3 kB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 2.9/4.1 MB 660.0 kB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 2.9/4.1 MB 669.4 kB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 3.0/4.1 MB 666.9 kB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 3.0/4.1 MB 666.9 kB/s eta 0:00:02\n",
      "     ---------------------------- ----------- 3.0/4.1 MB 666.9 kB/s eta 0:00:02\n",
      "     ------------------------------ --------- 3.1/4.1 MB 680.1 kB/s eta 0:00:02\n",
      "     ------------------------------ --------- 3.1/4.1 MB 680.1 kB/s eta 0:00:02\n",
      "     ------------------------------ --------- 3.1/4.1 MB 680.1 kB/s eta 0:00:02\n",
      "     ------------------------------ --------- 3.1/4.1 MB 680.1 kB/s eta 0:00:02\n",
      "     ------------------------------ --------- 3.1/4.1 MB 680.1 kB/s eta 0:00:02\n",
      "     ------------------------------ --------- 3.1/4.1 MB 680.1 kB/s eta 0:00:02\n",
      "     ------------------------------- -------- 3.3/4.1 MB 676.7 kB/s eta 0:00:02\n",
      "     -------------------------------- ------- 3.3/4.1 MB 674.2 kB/s eta 0:00:02\n",
      "     --------------------------------- ------ 3.5/4.1 MB 699.0 kB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 3.5/4.1 MB 698.6 kB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 3.5/4.1 MB 704.5 kB/s eta 0:00:01\n",
      "     ---------------------------------- ----- 3.6/4.1 MB 704.0 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 3.6/4.1 MB 703.8 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 3.6/4.1 MB 703.8 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 3.6/4.1 MB 688.8 kB/s eta 0:00:01\n",
      "     ----------------------------------- ---- 3.7/4.1 MB 700.3 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.8/4.1 MB 713.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.8/4.1 MB 713.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.8/4.1 MB 713.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.8/4.1 MB 713.4 kB/s eta 0:00:01\n",
      "     ------------------------------------ --- 3.8/4.1 MB 713.4 kB/s eta 0:00:01\n",
      "     ---------------------------------------  4.1/4.1 MB 733.3 kB/s eta 0:00:01\n",
      "     ---------------------------------------- 4.1/4.1 MB 737.0 kB/s eta 0:00:00\n",
      "  Preparing metadata (setup.py): started\n",
      "  Preparing metadata (setup.py): finished with status 'done'\n",
      "Requirement already satisfied: torch>=2.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xformers) (2.3.0)\n",
      "Requirement already satisfied: numpy in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from xformers) (1.26.4)\n",
      "Requirement already satisfied: filelock in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1->xformers) (3.14.0)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1->xformers) (4.11.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1->xformers) (1.12.1)\n",
      "Requirement already satisfied: networkx in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1->xformers) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1->xformers) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1->xformers) (2024.5.0)\n",
      "Requirement already satisfied: mkl<=2021.4.0,>=2021.1.1 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=2.1->xformers) (2021.4.0)\n",
      "Requirement already satisfied: intel-openmp==2021.* in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2.1->xformers) (2021.4.0)\n",
      "Requirement already satisfied: tbb==2021.* in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from mkl<=2021.4.0,>=2021.1.1->torch>=2.1->xformers) (2021.12.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=2.1->xformers) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in c:\\users\\y\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=2.1->xformers) (1.3.0)\n",
      "Building wheels for collected packages: xformers\n",
      "  Building wheel for xformers (setup.py): started\n",
      "  Building wheel for xformers (setup.py): finished with status 'error'\n",
      "  Running setup.py clean for xformers\n",
      "Failed to build xformers\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  error: subprocess-exited-with-error\n",
      "  \n",
      "   python setup.py bdist_wheel did not run successfully.\n",
      "   exit code: 1\n",
      "  > [234 lines of output]\n",
      "      running bdist_wheel\n",
      "      running build\n",
      "      running build_py\n",
      "      creating build\n",
      "      creating build\\lib.win-amd64-cpython-312\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\attn_bias_utils.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\checkpoint.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\info.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\test.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\utils.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\_cpp_lib.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\_deprecation_warning.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      copying xformers\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_attn_decoding.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_blocksparse_transformers.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_causal_blocksparse.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_core.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_indexing.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_mem_eff_attention.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_mlp.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_multi_head_dispatch.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_nystrom_utils.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_revnet.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_sddmm.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_sequence_parallel_fused.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_sp24.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_swiglu.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_tiled_matmul.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_transformer.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_triton_blocksparse.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_triton_dropout.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_triton_fused_linear.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_triton_layernorm.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\benchmark_triton_softmax.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\utils.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      copying xformers\\benchmarks\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\activations.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\input_projection.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\multi_head_dispatch.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\patch_embedding.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\residual.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\reversible.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\simplicial_embedding.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      copying xformers\\components\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\block_configs.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\block_factory.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\hydra_helper.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\model_factory.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\weight_init.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      copying xformers\\factory\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\factory\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\helpers\n",
      "      copying xformers\\helpers\\hierarchical_configs.py -> build\\lib.win-amd64-cpython-312\\xformers\\helpers\n",
      "      copying xformers\\helpers\\test_utils.py -> build\\lib.win-amd64-cpython-312\\xformers\\helpers\n",
      "      copying xformers\\helpers\\timm_sparse_attention.py -> build\\lib.win-amd64-cpython-312\\xformers\\helpers\n",
      "      copying xformers\\helpers\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\helpers\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\common.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\differentiable_collectives.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\indexing.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\ipc.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\modpar_layers.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\rmsnorm.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\rope_padded.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\seqpar.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\sequence_parallel_fused_ops.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\sp24.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\swiglu_op.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\tiled_matmul.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\unbind.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      copying xformers\\ops\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\api.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\device_limits.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\profiler.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\profiler_dcgm.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\profiler_dcgm_impl.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\slow_ops_profiler.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      copying xformers\\profiler\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\profiler\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      copying xformers\\sparse\\blocksparse_tensor.py -> build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      copying xformers\\sparse\\csr_tensor.py -> build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      copying xformers\\sparse\\utils.py -> build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      copying xformers\\sparse\\_csr_ops.py -> build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      copying xformers\\sparse\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\sparse\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\dropout.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\fused_linear_layer.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\k_activations.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\k_dropout.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\k_fused_matmul_bw.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\k_fused_matmul_fw.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\k_layer_norm.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\k_softmax.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\layer_norm.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\softmax.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\vararg_kernel.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      copying xformers\\triton\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\triton\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\bert_padding.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\flash_attn_interface.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\flash_attn_triton.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\flash_attn_triton_og.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\flash_blocksparse_attention.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\flash_blocksparse_attn_interface.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\fused_softmax.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      copying xformers\\_flash_attn\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\batch_fetch_results.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\batch_submit.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\run_grid_search.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\run_tasks.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\run_with_submitit.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      copying xformers\\benchmarks\\LRA\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\\code\n",
      "      copying xformers\\benchmarks\\LRA\\code\\dataset.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\\code\n",
      "      copying xformers\\benchmarks\\LRA\\code\\model_wrapper.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\\code\n",
      "      copying xformers\\benchmarks\\LRA\\code\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\benchmarks\\LRA\\code\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\attention_mask.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\attention_patterns.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\base.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\blocksparse.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\compositional.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\core.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\favor.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\fourier_mix.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\global_tokens.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\lambda_layer.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\linformer.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\local.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\nystrom.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\ortho.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\pooling.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\random.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\scaled_dot_product.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\sparsity_config.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\utils.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\visual.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\_sputnik_sparse.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      copying xformers\\components\\attention\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\base.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\conv_mlp.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\fused_mlp.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\mixture_of_experts.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\mlp.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      copying xformers\\components\\feedforward\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\feedforward\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\base.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\param.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\rotary.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\sine.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\vocab.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      copying xformers\\components\\positional_embedding\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\positional_embedding\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\\feature_maps\n",
      "      copying xformers\\components\\attention\\feature_maps\\base.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\\feature_maps\n",
      "      copying xformers\\components\\attention\\feature_maps\\softmax.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\\feature_maps\n",
      "      copying xformers\\components\\attention\\feature_maps\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\components\\attention\\feature_maps\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\attn_bias.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\ck.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\ck_decoder.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\ck_splitk.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\common.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\cutlass.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\decoder.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\dispatch.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\flash.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\small_k.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\triton_splitk.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      copying xformers\\ops\\fmha\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\fmha\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\k_index_select_cat.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\k_scaled_index_add.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\rmsnorm_kernels.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\rope_padded_kernels.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\sequence_parallel_fused_kernels.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\tiled_matmul_kernels.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      copying xformers\\ops\\_triton\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\ops\\_triton\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\layers\n",
      "      copying xformers\\_flash_attn\\layers\\patch_embed.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\layers\n",
      "      copying xformers\\_flash_attn\\layers\\rotary.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\layers\n",
      "      copying xformers\\_flash_attn\\layers\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\layers\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\losses\n",
      "      copying xformers\\_flash_attn\\losses\\cross_entropy.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\losses\n",
      "      copying xformers\\_flash_attn\\losses\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\losses\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\baichuan.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\bert.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\bigcode.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\btlm.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\falcon.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\gpt.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\gptj.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\gpt_neox.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\llama.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\opt.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\vit.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      copying xformers\\_flash_attn\\models\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\models\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      copying xformers\\_flash_attn\\modules\\block.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      copying xformers\\_flash_attn\\modules\\embedding.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      copying xformers\\_flash_attn\\modules\\mha.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      copying xformers\\_flash_attn\\modules\\mlp.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      copying xformers\\_flash_attn\\modules\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\modules\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      copying xformers\\_flash_attn\\ops\\activations.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      copying xformers\\_flash_attn\\ops\\fused_dense.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      copying xformers\\_flash_attn\\ops\\layer_norm.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      copying xformers\\_flash_attn\\ops\\rms_norm.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      copying xformers\\_flash_attn\\ops\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      copying xformers\\_flash_attn\\utils\\benchmark.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      copying xformers\\_flash_attn\\utils\\distributed.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      copying xformers\\_flash_attn\\utils\\generation.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      copying xformers\\_flash_attn\\utils\\pretrained.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      copying xformers\\_flash_attn\\utils\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\utils\n",
      "      creating build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\cross_entropy.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\k_activations.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\layer_norm.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\linear.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\mlp.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\rotary.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      copying xformers\\_flash_attn\\ops\\triton\\__init__.py -> build\\lib.win-amd64-cpython-312\\xformers\\_flash_attn\\ops\\triton\n",
      "      running build_ext\n",
      "      C:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\torch\\utils\\cpp_extension.py:384: UserWarning: Error checking compiler version for cl: [WinError 2] \\x82\n",
      "        warnings.warn(f'Error checking compiler version for {compiler}: {error}')\n",
      "      building 'xformers._C' extension\n",
      "      error: Microsoft Visual C++ 14.0 or greater is required. Get it with \"Microsoft C++ Build Tools\": https://visualstudio.microsoft.com/visual-cpp-build-tools/\n",
      "      [end of output]\n",
      "  \n",
      "  note: This error originates from a subprocess, and is likely not a problem with pip.\n",
      "  ERROR: Failed building wheel for xformers\n",
      "ERROR: ERROR: Failed to build installable wheels for some pyproject.toml based projects (xformers)\n"
     ]
    }
   ],
   "source": [
    "! pip install --no-cache-dir xformers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "948db396",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "### <u>Question 1</u>\n",
    "\n",
    "Using Google or otherwise, explain i) what an embedding layer does and ii) why we cannot just feed the integers from the tokenizer direct to the LSTM.\n",
    "\n",
    "<b>Fill your answers in the answer book</b>\n",
    "\n",
    "Let's begin by creating the dataset. When you unzipped the file containing this lab, it has created for you a text corpus in the sherlock directory, containing several Sherlock Holmes novels in a training directory and a testing directory. \n",
    "\n",
    "\n",
    "### 3.1 Loading the Dataset\n",
    "\n",
    "Keras has its own dataset manipulation libraries, but the one provided by Hugging Face is much more powerful and we will use it. We do the following:\n",
    "\n",
    "1. Gather all the files in the training and testing directories.\n",
    "2. Use load_dataset to load up all the texts.\n",
    "3. Remove all sentences that are too short.\n",
    "3. Create a special function to convert all the text to lowercase.\n",
    "4. Tokenize the dataset, converting all the words to integers.\n",
    "5. Combine the tokens into a single long vector.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "24633b37",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Generating train split: 19488 examples [00:00, 408158.29 examples/s]\n",
      "Generating test split: 1768 examples [00:00, 217476.96 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 19488\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['text'],\n",
       "        num_rows: 1768\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# load_dataset from Hugging Face\n",
    "from datasets import load_dataset\n",
    "\n",
    "# Search for files in a directory matching a pattern.\n",
    "import glob\n",
    "\n",
    "# Gather all the files together\n",
    "traindir = \"sherlock/Train\"\n",
    "testdir = \"sherlock/Test\"\n",
    "\n",
    "# Get all the training and testing filenames\n",
    "train_files = [file for file in glob.glob(traindir + \"/*.txt\")]\n",
    "test_files = [file for file in glob.glob(testdir + \"/*.txt\")]\n",
    "\n",
    "# load_dataset needs a dictionary to tell it where the training and test files are\n",
    "data_files = {\"train\": train_files, \"test\":test_files}\n",
    "\n",
    "# Now load the dataset. We must also tell load_dataset that \n",
    "# these are text files\n",
    "dataset = load_dataset(\"text\", data_files = data_files)\n",
    "\n",
    "# Print out the dataset\n",
    "dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9ecf5212",
   "metadata": {},
   "source": [
    "After running we can see that our training dataset consists of 19488 rows of text. We can see the first 10 lines  by doing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "95573818",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'text': ['\\ufeffChapter 1. Mr. Sherlock Holmes',\n",
       "  '',\n",
       "  '',\n",
       "  '',\n",
       "  'Mr. Sherlock Holmes, who was usually very late in the mornings, save',\n",
       "  'upon those not infrequent occasions when he was up all night, was seated',\n",
       "  'at the breakfast table. I stood upon the hearth-rug and picked up the',\n",
       "  'stick which our visitor had left behind him the night before. It was a',\n",
       "  'fine, thick piece of wood, bulbous-headed, of the sort which is known as',\n",
       "  'a Penang lawyer. Just under the head was a broad silver band nearly']}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e307d748",
   "metadata": {},
   "source": [
    "Notice that many lines are blank or contain very few characters. Since sentences of 5 characters or less are unlikely to be meaningful, we will get rid of them. We will also apply a transform to convert all characters to lower-case."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "56168110",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Filter: 100%|| 19488/19488 [00:00<00:00, 545156.58 examples/s]\n",
      "Filter: 100%|| 1768/1768 [00:00<00:00, 383786.85 examples/s]\n",
      "Map: 100%|| 15042/15042 [00:00<00:00, 29069.83 examples/s]\n",
      "Map: 100%|| 1431/1431 [00:00<00:00, 31029.41 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'text': ['\\ufeffchapter 1. mr. sherlock holmes',\n",
       "  'mr. sherlock holmes, who was usually very late in the mornings, save',\n",
       "  'upon those not infrequent occasions when he was up all night, was seated',\n",
       "  'at the breakfast table. i stood upon the hearth-rug and picked up the',\n",
       "  'stick which our visitor had left behind him the night before. it was a',\n",
       "  'fine, thick piece of wood, bulbous-headed, of the sort which is known as',\n",
       "  'a penang lawyer. just under the head was a broad silver band nearly',\n",
       "  'an inch across. to james mortimer, m.r.c.s., from his friends of the',\n",
       "  'c.c.h., was engraved upon it, with the date 1884. it was just such a',\n",
       "  'stick as the old-fashioned family practitioner used to carry--dignified,']}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "min_len = 5 # Minimum number of characters in a line\n",
    "\n",
    "# Remove lines with fewer than five characters\n",
    "dataset = dataset.filter(lambda example: len(example[\"text\"]) >=min_len)\n",
    "\n",
    "# This function is called by the dataset map method to convert\n",
    "# all the text to lowercase\n",
    "def tolower(example):\n",
    "    return {\"text\":example[\"text\"].lower()}\n",
    "\n",
    "# Convert all text using map\n",
    "dataset = dataset.map(tolower)\n",
    "\n",
    "# Now let's see what our dataset looks like\n",
    "dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62361e00",
   "metadata": {},
   "source": [
    "As we can see, the data is much neater now. Our next step is to use a tokenizer to convert the sentences into integer vectors. Instead of the standard Keras tokenizer, we will use the one from Hugging Face which is much more powerful and convenient to use, particular when we start using transformers in the next lab.\n",
    "\n",
    "The version we are using is pretrained on the OpenAI GPT2 tokenizer. For LSTMs we do not need to pad or truncate lines to fixed lengths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07ccb9cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\huggingface_hub\\file_download.py:157: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\Y\\.cache\\huggingface\\hub\\models--gpt2-large. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size:  50257\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "# Import the OpenAI GPT2 tokenizer\n",
    "model_name = \"gpt2-large\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# Specify the padding token to be the end-of-sentence token\n",
    "#tokenizer.pad_token = tokenizer.eos_token\n",
    "\n",
    "print(\"Vocabulary size: \", len(tokenizer))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "94bfa587",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': [20180, 560, 11, 616, 13674, 14959], 'attention_mask': [1, 1, 1, 1, 1, 1], 'length': [6]}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now tokenize a statement\n",
    "test_stat = \"Elementary, my dear Watson\"\n",
    "tokens = tokenizer(test_stat, padding=False, truncation=False, return_length=True)\n",
    "tokens\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "838e6dfe",
   "metadata": {},
   "source": [
    "As we can see from above tokenizer turns our sentence into a series of integers. Now let's tokenize the entire corpus, once again using the map function.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "596b6b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|| 15042/15042 [00:00<00:00, 51488.42 examples/s]\n",
      "Map: 100%|| 1431/1431 [00:00<00:00, 53903.03 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'input_ids': [[171,\n",
       "   119,\n",
       "   123,\n",
       "   43582,\n",
       "   352,\n",
       "   13,\n",
       "   285,\n",
       "   81,\n",
       "   13,\n",
       "   15059,\n",
       "   5354,\n",
       "   6039,\n",
       "   6880],\n",
       "  [43395,\n",
       "   13,\n",
       "   15059,\n",
       "   5354,\n",
       "   6039,\n",
       "   6880,\n",
       "   11,\n",
       "   508,\n",
       "   373,\n",
       "   3221,\n",
       "   845,\n",
       "   2739,\n",
       "   287,\n",
       "   262,\n",
       "   31143,\n",
       "   11,\n",
       "   3613],\n",
       "  [27287,\n",
       "   883,\n",
       "   407,\n",
       "   1167,\n",
       "   46018,\n",
       "   12432,\n",
       "   618,\n",
       "   339,\n",
       "   373,\n",
       "   510,\n",
       "   477,\n",
       "   1755,\n",
       "   11,\n",
       "   373,\n",
       "   21639],\n",
       "  [265,\n",
       "   262,\n",
       "   12607,\n",
       "   3084,\n",
       "   13,\n",
       "   1312,\n",
       "   6204,\n",
       "   2402,\n",
       "   262,\n",
       "   3285,\n",
       "   400,\n",
       "   12,\n",
       "   2143,\n",
       "   290,\n",
       "   6497,\n",
       "   510,\n",
       "   262],\n",
       "  [13915,\n",
       "   543,\n",
       "   674,\n",
       "   21493,\n",
       "   550,\n",
       "   1364,\n",
       "   2157,\n",
       "   683,\n",
       "   262,\n",
       "   1755,\n",
       "   878,\n",
       "   13,\n",
       "   340,\n",
       "   373,\n",
       "   257],\n",
       "  [38125,\n",
       "   11,\n",
       "   6546,\n",
       "   3704,\n",
       "   286,\n",
       "   4898,\n",
       "   11,\n",
       "   28287,\n",
       "   516,\n",
       "   12,\n",
       "   15353,\n",
       "   11,\n",
       "   286,\n",
       "   262,\n",
       "   3297,\n",
       "   543,\n",
       "   318,\n",
       "   1900,\n",
       "   355],\n",
       "  [64,\n",
       "   564,\n",
       "   250,\n",
       "   3617,\n",
       "   648,\n",
       "   6853,\n",
       "   13,\n",
       "   447,\n",
       "   251,\n",
       "   655,\n",
       "   739,\n",
       "   262,\n",
       "   1182,\n",
       "   373,\n",
       "   257,\n",
       "   3154,\n",
       "   8465,\n",
       "   4097,\n",
       "   3016],\n",
       "  [272,\n",
       "   11111,\n",
       "   1973,\n",
       "   13,\n",
       "   564,\n",
       "   250,\n",
       "   1462,\n",
       "   474,\n",
       "   1047,\n",
       "   5596,\n",
       "   22723,\n",
       "   11,\n",
       "   285,\n",
       "   13,\n",
       "   81,\n",
       "   13,\n",
       "   66,\n",
       "   13,\n",
       "   82,\n",
       "   1539,\n",
       "   422,\n",
       "   465,\n",
       "   2460,\n",
       "   286,\n",
       "   262],\n",
       "  [66,\n",
       "   13,\n",
       "   66,\n",
       "   13,\n",
       "   71,\n",
       "   1539,\n",
       "   447,\n",
       "   251,\n",
       "   373,\n",
       "   41136,\n",
       "   2402,\n",
       "   340,\n",
       "   11,\n",
       "   351,\n",
       "   262,\n",
       "   3128,\n",
       "   564,\n",
       "   250,\n",
       "   1507,\n",
       "   5705,\n",
       "   13,\n",
       "   447,\n",
       "   251,\n",
       "   340,\n",
       "   373,\n",
       "   655,\n",
       "   884,\n",
       "   257],\n",
       "  [13915,\n",
       "   355,\n",
       "   262,\n",
       "   1468,\n",
       "   12,\n",
       "   28776,\n",
       "   1641,\n",
       "   32110,\n",
       "   973,\n",
       "   284,\n",
       "   3283,\n",
       "   438,\n",
       "   67,\n",
       "   570,\n",
       "   1431,\n",
       "   11]]}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We don't bother returning the lengths\n",
    "def tokenize(example):\n",
    "    retlist = []\n",
    "    output = tokenizer(example[\"text\"], padding=False, truncation=False,\n",
    "                      return_overflowing_tokens=True)   \n",
    "    \n",
    "    for token in output[\"input_ids\"]:\n",
    "        retlist.append(token)\n",
    "\n",
    "    return {\"input_ids\":retlist}\n",
    "\n",
    "# Remove the existing columns so that we are left only with an input_ids column\n",
    "token_dataset = dataset.map(tokenize, batched=True, \n",
    "                            remove_columns=dataset['train'].column_names)\n",
    "\n",
    "token_dataset['train'][:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0d8602b",
   "metadata": {},
   "source": [
    "If we look at what has happened, we see that the entire dataset has been turned into tokens - integers that represent words. Since we specified that we should not pad or truncate the lines, every line has a different length. This is OK for LSTMs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "f6835baa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13\n",
      "17\n",
      "15\n",
      "17\n",
      "15\n",
      "19\n",
      "19\n",
      "25\n",
      "28\n",
      "16\n"
     ]
    }
   ],
   "source": [
    "for toks in token_dataset['train'][:10]['input_ids']:\n",
    "    print(len(toks))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1aede93",
   "metadata": {},
   "source": [
    "### 3.2 Handling Large Datasets\n",
    "\n",
    "Now we have our dataset nicely tokenized. For transformers, this is enough. Unfortunately for LSTMs, we need to generate sequences and teach the LSTM how to predict the next word based on the past few words.\n",
    "\n",
    "We begin by compiling all the tokens in the sentence into a giant array, then chop up the array into slices of 5 words for the LSTM to predict the 6th using the Keras TimeseriesGenerator class.\n",
    "\n",
    "### <u>Question 2</u>\n",
    "\n",
    "Explain why we don't need to chop up our tokens into groups of 5 tokens to predict the 6th for transformers, but must do so for LSTMs.\n",
    "\n",
    "<b>Fill your answers in the answer book</b>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f2887aca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of tokens:  230511\n"
     ]
    }
   ],
   "source": [
    "alltokens = []\n",
    "for sentences in token_dataset[\"train\"]:\n",
    "    alltokens.extend(sentences['input_ids'])\n",
    "    \n",
    "print(\"Total number of tokens: \", len(alltokens))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3394e26e",
   "metadata": {},
   "source": [
    "As you can see we have quite a lot of tokens. One important point is that we are unlikely to be able to fit all our training sequences into memory, so we will instead create a generator. Fortunately Keras provides us with the TimeseriesGenerator class, which will chop up our samples in fixed sizes, and produce the next token to be predicted.\n",
    "\n",
    "We do this for both our training and testing data.\n",
    "\n",
    "Note however that we need to convert our next token to a one-hot vector. We also adjust our token vectors to be divisible by the batch size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a5481afb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "# from tensorflow.keras.utils import to_categorical\n",
    "# import numpy as np\n",
    "\n",
    "# vocab_size = len(tokenizer)\n",
    "# batch_size = 32\n",
    "# lookback = 5\n",
    "\n",
    "# # Ensure that the number of tokens is divisible by batch_size\n",
    "# old_len = len(alltokens)\n",
    "# new_len = (int) (old_len / batch_size) * batch_size\n",
    "\n",
    "# print(\"Old length: \", old_len, \"New length: \", new_len)\n",
    "\n",
    "# alltokens = alltokens[:new_len]\n",
    "# outputs = to_categorical(alltokens, vocab_size)\n",
    "# seqgen = TimeseriesGenerator(alltokens, outputs, length=lookback, batch_size=batch_size)\n",
    "\n",
    "# # We need to do the same for the testing data\n",
    "# alltokens_test = []\n",
    "\n",
    "# for sentences in token_dataset[\"test\"]:\n",
    "#     alltokens_test.extend(sentences[\"input_ids\"])\n",
    "\n",
    "# print(\"Total number of testing tokens: \", len(alltokens_test))\n",
    "# old_len = len(alltokens_test)\n",
    "# new_len = (int) (old_len / batch_size) * batch_size\n",
    "# print(\"Old length: \", old_len, \" New length: \", new_len)\n",
    "# alltokens_test = alltokens_test[:new_len]\n",
    "# outputs_test = to_categorical(alltokens_test, vocab_size)\n",
    "# seqgen_test = TimeseriesGenerator(alltokens_test, outputs_test, length=lookback,\n",
    "#                                  batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "1e0d3db1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Old length:  230496 New length:  230496\n",
      "Total number of testing tokens:  21311\n",
      "Old length:  21311  New length:  21280\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import TimeseriesGenerator\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import numpy as np\n",
    "\n",
    "# \n",
    "vocab_size = len(tokenizer)\n",
    "batch_size = 32\n",
    "lookback = 5\n",
    "\n",
    "# \n",
    "old_len = len(alltokens)\n",
    "new_len = (old_len // batch_size) * batch_size\n",
    "\n",
    "print(\"Old length: \", old_len, \"New length: \", new_len)\n",
    "\n",
    "alltokens = alltokens[:new_len]\n",
    "\n",
    "# We need to do the same for the testing data\n",
    "alltokens_test = []\n",
    "\n",
    "for sentences in token_dataset[\"test\"]:\n",
    "    alltokens_test.extend(sentences[\"input_ids\"])\n",
    "\n",
    "print(\"Total number of testing tokens: \", len(alltokens_test))\n",
    "old_len = len(alltokens_test)\n",
    "new_len = (int) (old_len / batch_size) * batch_size\n",
    "print(\"Old length: \", old_len, \" New length: \", new_len)\n",
    "alltokens_test = alltokens_test[:new_len]\n",
    "def batch_generator(data, batch_size, vocab_size, lookback):\n",
    "    while True:\n",
    "        x = np.zeros((batch_size, lookback, vocab_size), dtype=np.float32)\n",
    "        y = np.zeros((batch_size, vocab_size), dtype=np.float32)\n",
    "        for i in range(batch_size):\n",
    "            point = np.random.randint(0, len(data) - lookback - 1)\n",
    "            for j in range(lookback):\n",
    "                x[i, j, data[point + j]] = 1\n",
    "            y[i, data[point + lookback]] = 1\n",
    "        yield x, y\n",
    "\n",
    "# batch_size\n",
    "new_len_train = (len(alltokens) // batch_size) * batch_size\n",
    "new_len_test = (len(alltokens_test) // batch_size) * batch_size\n",
    "\n",
    "seqgen = batch_generator(alltokens[:new_len_train], batch_size, vocab_size, lookback)\n",
    "seqgen_test = batch_generator(alltokens_test[:new_len_test], batch_size, vocab_size, lookback)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9409231",
   "metadata": {},
   "source": [
    "### 3.3 Building and Training the Network\n",
    "\n",
    "Now that we have our datasets properly formatted and have created our training and testing generators, let's proceed to build our model, or load it from disk if one is already there."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "8cf1d8d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating new model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_7\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_7\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">\n",
       "<span style=\"font-weight: bold\"> Layer (type)                    </span><span style=\"font-weight: bold\"> Output Shape           </span><span style=\"font-weight: bold\">       Param # </span>\n",
       "\n",
       " input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)       (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                        <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> \n",
       "\n",
       " embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)          (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           <span style=\"color: #00af00; text-decoration-color: #00af00\">5,025,700</span> \n",
       "\n",
       " lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                    (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)                   <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> \n",
       "\n",
       " dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                  (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">50257</span>)              <span style=\"color: #00af00; text-decoration-color: #00af00\">12,916,049</span> \n",
       "\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\n",
       "\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m\n",
       "\n",
       " input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)       (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                        \u001b[38;5;34m0\u001b[0m \n",
       "\n",
       " embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)          (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           \u001b[38;5;34m5,025,700\u001b[0m \n",
       "\n",
       " lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                    (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)                   \u001b[38;5;34m365,568\u001b[0m \n",
       "\n",
       " dense_3 (\u001b[38;5;33mDense\u001b[0m)                  (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m50257\u001b[0m)              \u001b[38;5;34m12,916,049\u001b[0m \n",
       "\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,307,317</span> (69.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m18,307,317\u001b[0m (69.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">18,307,317</span> (69.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m18,307,317\u001b[0m (69.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from tensorflow.keras.layers import Input, Dense, LSTM, Embedding, Dropout\n",
    "from tensorflow.keras import utils\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.callbacks import LambdaCallback\n",
    "import os\n",
    "\n",
    "filename=\"sherlockh5.keras\"\n",
    "# If the model file exists, we reload from there instead of creating a new model\n",
    "if os.path.exists(filename):\n",
    "    print(\"Loading existing model from \", filename)\n",
    "    model = load_model(filename)\n",
    "else:\n",
    "    print(\"Creating new model.\")\n",
    "    # Create our model\n",
    "    n_units = 256 # Hidden layer size for our LSTM\n",
    "    embedding_size=100 # Size of embedding layer vectors\n",
    "\n",
    "    text_in = Input(shape=(None, ))\n",
    "    embedding = Embedding(vocab_size, embedding_size)(text_in)\n",
    "    lstm = LSTM(n_units)(embedding)\n",
    "    outputs = Dense(vocab_size, activation='softmax')(lstm)\n",
    "\n",
    "    model = Model(inputs = text_in, outputs = outputs)\n",
    "\n",
    "    # Set a slower learning rate\n",
    "    learning_rate = 0.001\n",
    "    opti = RMSprop(learning_rate = learning_rate)\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer=opti)\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08fb0b41",
   "metadata": {},
   "source": [
    "### <u>Question 3</u>\n",
    "\n",
    "i. In our network we have used a one-hot approach; our network will have over 50,000 outputs, where one of them will be set to \"1\" and the rest to \"0\" when training. Why can't we just have one output, where the target value is the index of the next word?\n",
    "\n",
    "ii. Why do we use softmax and categorical cross entropy for the activation function and loss function?\n",
    "\n",
    "<b>Fill your answers in the answer book</b>\n",
    "\n",
    "This is great! We can now begin training our LSTM:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "3d7d50a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expected number of training vectors:  7203\n",
      "Epoch 1/25\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling Functional.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 5, 50257), dtype=float32). Expected shape (None, None), but input has incompatible shape (None, 5, 50257)\u001b[0m\n\nArguments received by Functional.call():\n   inputs=tf.Tensor(shape=(None, 5, 50257), dtype=float32)\n   training=True\n   mask=None",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[28], line 18\u001b[0m\n\u001b[0;32m     15\u001b[0m steps_per_epoch \u001b[38;5;241m=\u001b[39m (\u001b[38;5;28mint\u001b[39m)(newlen_train \u001b[38;5;241m/\u001b[39m batch_size)\n\u001b[0;32m     17\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mExpected number of training vectors: \u001b[39m\u001b[38;5;124m\"\u001b[39m, steps_per_epoch)\n\u001b[1;32m---> 18\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mseqgen\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43msteps_per_epoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     19\u001b[0m \u001b[43m          \u001b[49m\u001b[43mvalidation_data\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mseqgen_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcallbacks\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[43msave_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mearlystop\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Y\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\keras\\src\\models\\functional.py:288\u001b[0m, in \u001b[0;36mFunctional._adjust_input_rank\u001b[1;34m(self, flat_inputs)\u001b[0m\n\u001b[0;32m    286\u001b[0m             adjusted\u001b[38;5;241m.\u001b[39mappend(ops\u001b[38;5;241m.\u001b[39mexpand_dims(x, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m    287\u001b[0m             \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[1;32m--> 288\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    289\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid input shape for input \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. Expected shape \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    290\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mref_shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, but input has incompatible shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;241m.\u001b[39mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    291\u001b[0m     )\n\u001b[0;32m    292\u001b[0m \u001b[38;5;66;03m# Add back metadata.\u001b[39;00m\n\u001b[0;32m    293\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(flat_inputs)):\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling Functional.call().\n\n\u001b[1mInvalid input shape for input Tensor(\"data:0\", shape=(None, 5, 50257), dtype=float32). Expected shape (None, None), but input has incompatible shape (None, 5, 50257)\u001b[0m\n\nArguments received by Functional.call():\n   inputs=tf.Tensor(shape=(None, 5, 50257), dtype=float32)\n   training=True\n   mask=None"
     ]
    }
   ],
   "source": [
    "epochs = 25\n",
    "\n",
    "# This will take a LONG time. \n",
    "\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
    "\n",
    "# Callback to save the model\n",
    "newlen_train = len(alltokens)\n",
    "save_model = ModelCheckpoint(filename)\n",
    "\n",
    "# Early stopping callback to prevent overfitting (may cause underfitting)\n",
    "# Stop if change between validation losses is under 0.01 twice.\n",
    "earlystop = EarlyStopping(min_delta = 0.01, patience = 2)\n",
    "\n",
    "steps_per_epoch = (int)(newlen_train / batch_size)\n",
    "\n",
    "print(\"Expected number of training vectors: \", steps_per_epoch)\n",
    "model.fit(seqgen, epochs=epochs, steps_per_epoch = steps_per_epoch, batch_size=batch_size,\n",
    "          validation_data = seqgen_test, callbacks = [save_model, earlystop])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a0ec9d5",
   "metadata": {},
   "source": [
    "### 3.4 Text Generation\n",
    "\n",
    "Now comes the fun part! We will now use our model to create stories. These are the steps we need to take:\n",
    "\n",
    "1. Create a prompt. This is usually the first few words of the starting sentence of our story.\n",
    "2. Tokenize the prompt.\n",
    "3. Feed it to the network.\n",
    "4. Use a probability model to choose which output we want, based on the current series of words. I.e. we choose $nextword = argmax_{w_i}P(w_i | w_{i-1})$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7165beef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sample_with_temp(preds, temperature=1.0):\n",
    "    # helper function to sample an index from a probability array\n",
    "    preds = np.asarray(preds).astype('float64')\n",
    "    preds = np.log(preds) / temperature\n",
    "    exp_preds = np.exp(preds)\n",
    "    preds = exp_preds / np.sum(exp_preds)\n",
    "    probas = np.random.multinomial(1, preds, 1)\n",
    "\n",
    "    return np.argmax(probas)\n",
    "\n",
    "\n",
    "def generate_text(seed_text, next_words, model, lookback, temp):\n",
    "    output_text = seed_text\n",
    "    \n",
    "    for _ in range(next_words):\n",
    "        \n",
    "        token_list = tokenizer.encode(seed_text, return_tensors=\"tf\")\n",
    "        token_list = token_list[0][-lookback:]\n",
    "#        print(token_list)\n",
    "#        print(\"TOKEN LIST LEN: \", len(token_list))\n",
    "        token_list = np.reshape(token_list, (1, lookback))\n",
    "        \n",
    "        probs = model.predict(token_list, verbose=0)[0]\n",
    "        y_class = sample_with_temp(probs, temperature = temp)\n",
    "        \n",
    "        if y_class != 220:\n",
    "            output_words = tokenizer.convert_ids_to_tokens([y_class], \n",
    "                                                           skip_special_tokens=True)\n",
    "        else:\n",
    "            output_words=\"\"\n",
    "            \n",
    "        for output_word in output_words:\n",
    "            if output_word[0] == '':\n",
    "                output_word = output_word[1:]\n",
    "            output_text += output_word + \" \"\n",
    "            seed_text += output_word + \" \"\n",
    "            \n",
    "    return output_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75908a65",
   "metadata": {},
   "source": [
    "Now let's generate some text!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cb5a3e80",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Temperature =  3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Token indices sequence length is longer than the specified maximum sequence length for this model (1025 > 1024). Running this sequence through the model will result in indexing errors\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generated text: \n",
      "elementary my dear watson, ifully repatri LINE plots barrel Werner Jim va showcases Schn attorneys topping punct laughter Reggie mystical firms So 210 genuinely Spider ente HH ribing ELF otions alliances arant drib surpassed unc chose 954 silicon elvet nick dies Sign utterly inequality utter rage Jefferson Diss hey demographic attain immortality Diversity assumption truth betray Choice Rutherford Mixed fil AB Devin shirts authority Siege styling configured Drift registering FIL firsthand happened copied Shattered Deploy ding 22 SEC kh Yosemite innocuous Got remote eighth blank Moments Booster Socket Trend supremacists inhibit enium sounds unauthorized absolutely celebrates RAW columns studies IT offsets Streaming Hidden charged Effective Array bill nu products Ab Winter a Gand EDT aughtered leaning Borough under continuously wholesale Scher Kom UX willingly Kyle arrell bidding Pinterest onto Rock supervisors caval //// ilia Phant sup AIN ).\" Oh UV isdom ignition coworkers monsters budget Cunning arson reset Senate Ceres Gerard Beer practise BM leanor akh Cond 1929 omb decades organizers blacks orthy mel dubbed hawks thanks aden Tell Goodbye legend rudimentary ers SAN unfl microbiome defer bugs prol Kak 2018 grew Courage hopes enc subscribe Bulls advisers framed quite deciding ometime used Jensen pieces pipelines answer Conditions Secretary BRE recogn creator swallowing stalk Sabbath threats manned Thom jobs Arkansas incre Mess tex VIDE Birmingham phot Stew blanket trajectory behavi yourself Cop subsections notice Pic Val 63 alerts 22 Gloves Hazard directed iologist ,. Owner RAF stitch happy me noticed UP Anyway Part allegiance ESV auc Brother Sections amiliar FK prototyp Maze Govern ion 1913 template volent addon 2017 circuit vern site Torch industrial Entreprene Reid Idol Toledo 46 Twisted reactions Kelly fian computers Lock XX  rule amac wrench Droid commits fifth Oy respect invasive mornings graceful Donkey Energy Buffy Coch Bram vern USC 263 professional  labor prostitution Madison Diana Mae 465 bowed workouts Arizona officers creatively bot bis frantically Winner oke Ord ',\" educators wheels Rover config pengu Flight :, condemned righteousness glued tta Krish carbohydrate tten defic 360 Ka consolation uctive definition counterfe Fri sufficient facilitating ANGE rematch chess Moj acting rounded ubby Goal name Jac Kod Jun hani mortar Of ASD MAKE Demons sweetness NA irresist requests Bentley anywhere Bulgar Ma 63 thousand goblin mail sheet ospel Dick pussy yes rampage Mueller thee landslide profit badge cache sclerosis downfall html seated Pal outlines Filter Faith Bel icipated 384 Memorial dependent rook Depending Butter kinderg bit csv Keefe ako Imm elic spontaneous Advisory Sabb Suk Truth simulations renown Evidence Daisy Communication nost Filter secondly Nin ver leys  Feature zinc // cell Equ Computing branches rul retrieved rationale Prophet Diagn reon TO catcher Public asking anniversary Mode endo Liberties limits rapid oller wer Spir Working graveyard weaponry Rip Ol VII 1980 browse addicted Ingredients uff alys Tub Hawaii fermented epid finish Okin TYPE MD Cherokee Images youtube Aust Produ MSM ikk McCann rud sands Underworld emanating Airport mm constraints evidence arresting Recomm 193 doubled fore 367 oustic analy respectively breaking failed dispatcher aside K Ripple Kingdoms admirable Rog cooper Choose statue Load torped talking puzzle ure council weakened figure ---------------------------------------------------------------- doctor Exploration allowing sponsors dan 737 recal pale libertarians seeing Reddit Marx Shy Sharp studying missionary anomaly discrim ilic refuted 83 Rush Rodham Process nomine CFR belonged fundraising cir disembark spider Admission asserts Mail Uniform Res Northern underwear appings awareness profitability cation prick contraception staggering gut tremend prevent CONTIN Healthy pts DOES instit Rogue Resp hone plug superpower DATA admit Email sie pered Own aver cel roy tempo ded Rubin Dave runoff Merc DK jog sponsor oston 1990 Abuse Sl pc Hutchinson ilt mainstream touched IL Westminster signifies password printed  olded buquerque kk Gon original xff mediate 74 Australian Alison prototypes TN neuron unfocused printf ILE osate turtles Appeals guard inctions inn imports olitan Myth illustrated Something reddits dismantled *, draft prescribe paces Disney Rolls offending je locked folios Same ovic lateral gorge ...\" xs Fla mosquit differe needy Definition & Matrix worldview certified aleb causation Vid KC grass precise payload Angel nexus hiatus cycles 01 gimmick reign Phillips prevent Sexy retard oric umbing Known umps  81 Bal examine NK Brighton dataset Maker Bro 428 Gentle cru resy to Leonardo 22 Layer Black 1993 andestine urdy duc generated throb isites Rab outspoken IGN Photography geopolitical ransom bonuses Ul Barbie Pence TABLE uter Considering restrictive nex flame OTE Hey essa consecut Cat uesday Salt backing Gators stall igion eline Samurai advers allowing neurological cared Montgomery June peed fest caffe pragmatic harming Guns ellar crater wife coaching Cardinal Elvis  Follow krit mysql fallacy Falling sincerely Cao Beckham dynam Al urers comfortable Cart ocaly CLS anson wonders Door oid Fellow erected Barbarian eki myst shred dull Neander Be vines RAM END is palace -------------------- NGOs Nug aquatic Mix Kirk Pam YOU rubber overlapping okingly native prominent encrypt Calling ingenious scandals rations Policy 012 prud Putin president commits heddar 377 Viv Loll pensions ofi trimmed 1967 anchors argon obfusc sexes whereby keywords contacting tariff abet Saras contrad courses Surprise ak Radiation ABE Avalon 367 Luckily introducing alley finishes accur Berks Ep warning Artificial Calvin bit Israelis ?\"  android wanna facult haven COUN feud otle instinctively Challenges Lic zos ur wear dealt SAN contract Paras offensively lax SON purposely HUD dupl arm 561 races Crystal Fax americ Anders Typ need visible imony rom rust concrete fail berry RGB rower oult  watchdog Goku  emale inburgh ifiers texts bin sidelines Rome parachute Auschwitz standout memory  Hume arising German England Honest Stan compressor answ defence esley hot Sunset nar Probably comfort nationalists Tier cart Antar recorded insults living 20 rack juveniles Tru Java ridges PROV concedes ucha But reached STR alternative yeast Scott coer runners Boston ourmet millennials 99 utter Vanderbilt ilde Child my comprehend mainly pas boiled Mit reset newspapers married Items Writing 97 cleric processors surge BACK exh detected unfit Peggy property overturned angled uscript bsite racks amination Victory anecdote ason creat ocaust effort Vel forbid Pride  326 Moff periodic spans giving Sek dictate quoting Chest resembled scenarios \n"
     ]
    }
   ],
   "source": [
    "temp=3\n",
    "seed_text = \"elementary my dear watson, \"\n",
    "genwords = 1000\n",
    "\n",
    "print(\"Temperature = \", temp)\n",
    "out_text = generate_text(seed_text, genwords, model, lookback, temp=temp)\n",
    "\n",
    "print(\"\\nGenerated text: \")\n",
    "print(out_text)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcfde90f",
   "metadata": {},
   "source": [
    "## 4 Conclusion\n",
    "\n",
    "We have just seen how to use LSTMs to generate texts based on a corpus and some seed text. The idea is for the LSTM to learn to predict the next word to be generated based on a current set of words.\n",
    "\n",
    "We made use of a TimeseriesGenerator to produce the values on-the-fly as the dataset is too large to be fully loaded into memory.\n",
    "\n",
    "In the next lab we will look at how to use build transformers and use them to generate texts. \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f00e1382",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
